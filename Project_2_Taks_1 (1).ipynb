{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project 2_Taks 1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "c5e7027c",
        "2a4e137b",
        "BfQpIXR7dstK",
        "57ca912b",
        "363f6c07",
        "PoFshHIfwEW6",
        "ffDhnuxrwEXB",
        "u5g5rGWCwEXH",
        "Fn8ADkI4eQ_3",
        "yJ8UpZjbeQ_8",
        "scWKYJWfeRAB",
        "G6d6JELXeRAE",
        "xcA2m-G2CiT1",
        "IB80iOTCClPU",
        "sIyec6OyFHdL",
        "P3frCtEtvKzP",
        "9gXRtQdAxHkF",
        "y0gyX2LkyObX",
        "u30Z1M6z9kRL",
        "VguiXyRKy_ml",
        "NHXij6Tuo_mV"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b4296aec38164b7496621245c08b82c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5f5b50d1d8ae4edb9c0da05172ded165",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_19df89a42a064c3e98712a92b86e9705",
              "IPY_MODEL_04ae32793d6e4014bf9c91ebf8a8bc8c",
              "IPY_MODEL_d1de220393e243de952826290d00f97a"
            ]
          }
        },
        "5f5b50d1d8ae4edb9c0da05172ded165": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "19df89a42a064c3e98712a92b86e9705": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c66ddda8ec0c49b1805258a990d1f69c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "training routine: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_443aaccb37764d51ac444b8d049f58c6"
          }
        },
        "04ae32793d6e4014bf9c91ebf8a8bc8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_70e8fb64d1d74dcfb0b418bc3c890c0e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 300,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 300,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8525eba212424845bd0dac24f8c75565"
          }
        },
        "d1de220393e243de952826290d00f97a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_394fd9816bf640eebd1ecc46e633a65f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 300/300 [06:03&lt;00:00,  1.20s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9fc81d6896b74082916f79882bdaf6f4"
          }
        },
        "c66ddda8ec0c49b1805258a990d1f69c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "443aaccb37764d51ac444b8d049f58c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "70e8fb64d1d74dcfb0b418bc3c890c0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8525eba212424845bd0dac24f8c75565": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "394fd9816bf640eebd1ecc46e633a65f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9fc81d6896b74082916f79882bdaf6f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c44b3f54d84e453991f9f5a32608304e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_aeecac282ed449a3bfa83f56afb0f675",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2c9582248ac14368befb4b15f620c383",
              "IPY_MODEL_f46d4b7bbcc44d128bfef287cad6fc03",
              "IPY_MODEL_a907b8737d4f4c328a14106f5bc5fe55"
            ]
          }
        },
        "aeecac282ed449a3bfa83f56afb0f675": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2c9582248ac14368befb4b15f620c383": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d46dfc68643a49f983222849b965506e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "split=train:  90%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7d92b6f6087441b7ac872b0f663c7c53"
          }
        },
        "f46d4b7bbcc44d128bfef287cad6fc03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b72b006d65e446539497246c1cd9d8d7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 10,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 9,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_09a33735d32c428e812c7c410f341251"
          }
        },
        "a907b8737d4f4c328a14106f5bc5fe55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_97ab1e75a9544590b030b0c204638d64",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9/10 [06:03&lt;00:00,  8.21it/s, acc=82.7, epoch=299, loss=0.395]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_56638dfec384461f8634c8a4f5a6b275"
          }
        },
        "d46dfc68643a49f983222849b965506e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7d92b6f6087441b7ac872b0f663c7c53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b72b006d65e446539497246c1cd9d8d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "09a33735d32c428e812c7c410f341251": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "97ab1e75a9544590b030b0c204638d64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "56638dfec384461f8634c8a4f5a6b275": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "28be81ad5bce49a4a42f5fdab9c29b0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c2b9981ae1d64f26a7ba9ab8d1d06097",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3206065547054ed4b6fe9862e6aa3da5",
              "IPY_MODEL_207b758667084744823048741960e7e7",
              "IPY_MODEL_b6b3815303ca4b95bbbe2888c6b0de73"
            ]
          }
        },
        "c2b9981ae1d64f26a7ba9ab8d1d06097": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3206065547054ed4b6fe9862e6aa3da5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b2cbfc5e1da24a5587164d0c94b13635",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "split=val:  50%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_361a9b89bfd74367a1e4612dc8c6b3bd"
          }
        },
        "207b758667084744823048741960e7e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9ff55aadc1f841f8bb16a579c758b024",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_85c30d015cf146628ab50a6f1f62b5bc"
          }
        },
        "b6b3815303ca4b95bbbe2888c6b0de73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_555790f7b8544204aabee93e6e17cf1c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/2 [06:03&lt;00:01,  1.40s/it, acc=68, epoch=299, loss=0.763]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8443a4329b554a04afe71b5d44dfccbf"
          }
        },
        "b2cbfc5e1da24a5587164d0c94b13635": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "361a9b89bfd74367a1e4612dc8c6b3bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9ff55aadc1f841f8bb16a579c758b024": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "85c30d015cf146628ab50a6f1f62b5bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "555790f7b8544204aabee93e6e17cf1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8443a4329b554a04afe71b5d44dfccbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "970c8ac4",
        "outputId": "c9b14f9a-155f-4882-bcfb-16bba1a48c62"
      },
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "figure(figsize=(8, 8), dpi=80)\n",
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import scipy as sp\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "import nltk, nltk.data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import wordnet\n",
        "from tqdm.notebook import tqdm\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import TfidfModel\n",
        "from argparse import Namespace"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x640 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 30.9 ms (started: 2021-10-25 13:19:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5USvACzFkSGt",
        "outputId": "c2f77288-d88e-46a6-b36c-1c5122022114"
      },
      "source": [
        "%load_ext autotime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autotime extension is already loaded. To reload it, use:\n",
            "  %reload_ext autotime\n",
            "time: 3.2 ms (started: 2021-10-25 13:19:51 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9piPTv7qRFhc"
      },
      "source": [
        "# <center>Project 2 Natural Language for NLP</center>\n",
        "\n",
        "<center>   \n",
        "\n",
        "**Name_ Student number:**\n",
        "<br>\n",
        "Chanxin Xie_22566456\n",
        "<br>\n",
        "Yiyong Duan_22628103<br>\n",
        "\n",
        "**Date created:** Oct 5th 2022<br>\n",
        "**Last modified:** Oct 25th 2022<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylM59-hpfh1k"
      },
      "source": [
        "## This notebook contains all details about Task 1: Binary Document Classification:\n",
        "\n",
        "\n",
        "**This task follows the procedures listed below:**\n",
        "\n",
        "1. Initial Data preparation\n",
        "2. Data Prepreocessing\n",
        "2. Split the training/testing/validation dataset\n",
        "3. Perform word embedding\n",
        "4. Group data into minibatch for the model via DataLoader\n",
        "5. Train Model\n",
        "6. Evaluation\n",
        "7. Compare performance of two different models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8VwQpANuWv2",
        "outputId": "b5a78b2b-d70b-4fad-fa9e-c0b5584d7a4c"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/NLP/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/NLP\n",
            "time: 17.6 ms (started: 2021-10-25 13:19:51 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDkf_-2GvI5O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "32823fe8-b965-4f58-8c41-cc0b7b000d6a"
      },
      "source": [
        "# Import dataset\n",
        "df_org = pd.read_csv('us_data_2000.csv')\n",
        "df_org.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MINE_ID</th>\n",
              "      <th>CONTROLLER_ID</th>\n",
              "      <th>CONTROLLER_NAME</th>\n",
              "      <th>OPERATOR_ID</th>\n",
              "      <th>OPERATOR_NAME</th>\n",
              "      <th>CONTRACTOR_ID</th>\n",
              "      <th>DOCUMENT_NO</th>\n",
              "      <th>SUBUNIT_CD</th>\n",
              "      <th>SUBUNIT</th>\n",
              "      <th>ACCIDENT_DT</th>\n",
              "      <th>CAL_YR</th>\n",
              "      <th>CAL_QTR</th>\n",
              "      <th>FISCAL_YR</th>\n",
              "      <th>FISCAL_QTR</th>\n",
              "      <th>ACCIDENT_TIME</th>\n",
              "      <th>DEGREE_INJURY_CD</th>\n",
              "      <th>DEGREE_INJURY</th>\n",
              "      <th>FIPS_STATE_CD</th>\n",
              "      <th>UG_LOCATION_CD</th>\n",
              "      <th>UG_LOCATION</th>\n",
              "      <th>UG_MINING_METHOD_CD</th>\n",
              "      <th>UG_MINING_METHOD</th>\n",
              "      <th>MINING_EQUIP_CD</th>\n",
              "      <th>MINING_EQUIP</th>\n",
              "      <th>EQUIP_MFR_CD</th>\n",
              "      <th>EQUIP_MFR_NAME</th>\n",
              "      <th>EQUIP_MODEL_NO</th>\n",
              "      <th>SHIFT_BEGIN_TIME</th>\n",
              "      <th>CLASSIFICATION_CD</th>\n",
              "      <th>CLASSIFICATION</th>\n",
              "      <th>ACCIDENT_TYPE_CD</th>\n",
              "      <th>ACCIDENT_TYPE</th>\n",
              "      <th>NO_INJURIES</th>\n",
              "      <th>TOT_EXPER</th>\n",
              "      <th>MINE_EXPER</th>\n",
              "      <th>JOB_EXPER</th>\n",
              "      <th>OCCUPATION_CD</th>\n",
              "      <th>OCCUPATION</th>\n",
              "      <th>ACTIVITY_CD</th>\n",
              "      <th>ACTIVITY</th>\n",
              "      <th>INJURY_SOURCE_CD</th>\n",
              "      <th>INJURY_SOURCE</th>\n",
              "      <th>NATURE_INJURY_CD</th>\n",
              "      <th>NATURE_INJURY</th>\n",
              "      <th>INJ_BODY_PART_CD</th>\n",
              "      <th>INJ_BODY_PART</th>\n",
              "      <th>SCHEDULE_CHARGE</th>\n",
              "      <th>DAYS_RESTRICT</th>\n",
              "      <th>DAYS_LOST</th>\n",
              "      <th>TRANS_TERM</th>\n",
              "      <th>RETURN_TO_WORK_DT</th>\n",
              "      <th>IMMED_NOTIFY_CD</th>\n",
              "      <th>IMMED_NOTIFY</th>\n",
              "      <th>INVEST_BEGIN_DT</th>\n",
              "      <th>NARRATIVE</th>\n",
              "      <th>CLOSED_DOC_NO</th>\n",
              "      <th>COAL_METAL_IND</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100003</td>\n",
              "      <td>41044</td>\n",
              "      <td>Lhoist Group</td>\n",
              "      <td>L13586</td>\n",
              "      <td>Lhoist North America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.201210e+11</td>\n",
              "      <td>3</td>\n",
              "      <td>STRIP, QUARY, OPEN PIT</td>\n",
              "      <td>14/03/2012</td>\n",
              "      <td>2012</td>\n",
              "      <td>1</td>\n",
              "      <td>2012</td>\n",
              "      <td>2</td>\n",
              "      <td>945</td>\n",
              "      <td>5</td>\n",
              "      <td>DAYS RESTRICTED ACTIVITY ONLY</td>\n",
              "      <td>1</td>\n",
              "      <td>?</td>\n",
              "      <td>NO VALUE FOUND</td>\n",
              "      <td>?</td>\n",
              "      <td>NO VALUE FOUND</td>\n",
              "      <td>24</td>\n",
              "      <td>Front-end loader, Tractor-shovel, Payloader, H...</td>\n",
              "      <td>119</td>\n",
              "      <td>Not on this list</td>\n",
              "      <td>22321</td>\n",
              "      <td>600.0</td>\n",
              "      <td>12</td>\n",
              "      <td>POWERED HAULAGE</td>\n",
              "      <td>21</td>\n",
              "      <td>CGHT I, U, B, MVNG &amp; STTN OBJS</td>\n",
              "      <td>1</td>\n",
              "      <td>4.35</td>\n",
              "      <td>4.35</td>\n",
              "      <td>0.67</td>\n",
              "      <td>374</td>\n",
              "      <td>Warehouseman, Bagger, Palletizer/Stacker, Stor...</td>\n",
              "      <td>28</td>\n",
              "      <td>HANDLING SUPPLIES/MATERIALS</td>\n",
              "      <td>76</td>\n",
              "      <td>SURFACE MINING MACHINES</td>\n",
              "      <td>160</td>\n",
              "      <td>CONTUSN,BRUISE,INTAC SKIN</td>\n",
              "      <td>700</td>\n",
              "      <td>MULTIPLE PARTS (MORE THAN ONE MAJOR)</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>N</td>\n",
              "      <td>03/26/2012</td>\n",
              "      <td>?</td>\n",
              "      <td>NO VALUE FOUND</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Employee was cleaning up at the Primary Crushe...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100003</td>\n",
              "      <td>41044</td>\n",
              "      <td>Lhoist Group</td>\n",
              "      <td>L13586</td>\n",
              "      <td>Lhoist North America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.200700e+11</td>\n",
              "      <td>30</td>\n",
              "      <td>MILL OPERATION/PREPARATION PLANT</td>\n",
              "      <td>8/01/2007</td>\n",
              "      <td>2007</td>\n",
              "      <td>1</td>\n",
              "      <td>2007</td>\n",
              "      <td>2</td>\n",
              "      <td>1105</td>\n",
              "      <td>6</td>\n",
              "      <td>NO DYS AWY FRM WRK,NO RSTR ACT</td>\n",
              "      <td>1</td>\n",
              "      <td>?</td>\n",
              "      <td>NO VALUE FOUND</td>\n",
              "      <td>?</td>\n",
              "      <td>NO VALUE FOUND</td>\n",
              "      <td>28</td>\n",
              "      <td>Hand tools (not powered)</td>\n",
              "      <td>121</td>\n",
              "      <td>Not Reported</td>\n",
              "      <td>NaN</td>\n",
              "      <td>700.0</td>\n",
              "      <td>10</td>\n",
              "      <td>HANDTOOLS (NONPOWERED)</td>\n",
              "      <td>8</td>\n",
              "      <td>STRUCK BY, NEC</td>\n",
              "      <td>1</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.02</td>\n",
              "      <td>374</td>\n",
              "      <td>Warehouseman, Bagger, Palletizer/Stacker, Stor...</td>\n",
              "      <td>30</td>\n",
              "      <td>HAND TOOLS (NOT POWERED)</td>\n",
              "      <td>46</td>\n",
              "      <td>AXE,HAMMER,SLEDGE</td>\n",
              "      <td>180</td>\n",
              "      <td>CUT,LACER,PUNCT-OPN WOUND</td>\n",
              "      <td>100</td>\n",
              "      <td>HEAD,NEC</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>N</td>\n",
              "      <td>1/09/2007</td>\n",
              "      <td>?</td>\n",
              "      <td>NO VALUE FOUND</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Handle of sledgehammer broke and head of hamme...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   MINE_ID CONTROLLER_ID  ... CLOSED_DOC_NO COAL_METAL_IND\n",
              "0   100003         41044  ...           NaN              M\n",
              "1   100003         41044  ...           NaN              M\n",
              "\n",
              "[2 rows x 57 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 164
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 154 ms (started: 2021-10-25 13:19:51 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9ba2e6d"
      },
      "source": [
        "## 1 Initial Data preparation\n",
        "This task is to clean up the original dataset so that we can use the NARRATIVE text field to predict the degree of injury (DEGREE_INJURY). \n",
        "1. Initial cleaning up\n",
        "2. Class Determination -- We make use the degree of injury code (DEGREE_INJURY_CD) to group injuries into two categories and assign the corresponding class to each row. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6be90514"
      },
      "source": [
        "### 1.1 Clean up data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Suj7Ev5L0Ez7"
      },
      "source": [
        "As the goal is to use the NARRATIVE text field to predict the degree of injury (DEGREE_INJURY), we extract column 'DEGREE_INJURY_CD' and 'NARRATIVE' from the original dataset and remove the values in 'DEGREE_INJURY_CD' which cannot be classified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9760f320",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02e57f50-231f-4557-a2ef-dab16f903396"
      },
      "source": [
        "# subset data to get column 'DEGREE_INJURY_CD','NARRATIVE'\n",
        "df = df_org[['DEGREE_INJURY_CD','NARRATIVE']]\n",
        "# remove value '?' in column 'DEGREE_INJURY_CD'\n",
        "df = df[df.DEGREE_INJURY_CD != '?']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 12.5 ms (started: 2021-10-25 13:19:51 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13Afa7BTqPjj"
      },
      "source": [
        "### 1.2 Class Determination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOpYmjSY1QbP"
      },
      "source": [
        "#### 1.2.1 Investigation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYSPwcx_zJsv"
      },
      "source": [
        "From the pivot table below, we can see the relationship between 'DEGREE_INJURY' and their corresponding 'DEGREE_INJURY_CD' as well as their distribution. We notice that they could be classified by the degree of injury code. And they can be seperated roughly into two groups: severe and light."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "8c45a577",
        "outputId": "be2d7408-6986-4b27-ba26-83e49113acbd"
      },
      "source": [
        "df_pivot = df_org[['DEGREE_INJURY','DEGREE_INJURY_CD']]\n",
        "# Firstly, we check up the dataframe by creating a pivot table with DEGREE_INJURY_CD and DEGREE_INJURY.\n",
        "pd.pivot_table(df_pivot, values = 'DEGREE_INJURY_CD', index = 'DEGREE_INJURY', columns= ['DEGREE_INJURY_CD'], aggfunc=lambda x:len(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>DEGREE_INJURY_CD</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>10</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>?</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DEGREE_INJURY</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ACCIDENT ONLY</th>\n",
              "      <td>219.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ALL OTHER CASES (INCL 1ST AID)</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DAYS AWAY FROM WORK ONLY</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>595.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DAYS RESTRICTED ACTIVITY ONLY</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>359.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DYS AWY FRM WRK &amp; RESTRCTD ACT</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>145.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FATALITY</th>\n",
              "      <td>NaN</td>\n",
              "      <td>11.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>INJURIES DUE TO NATURAL CAUSES</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>INJURIES INVOLVNG NONEMPLOYEES</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NO DYS AWY FRM WRK,NO RSTR ACT</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>552.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NO VALUE FOUND</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>OCCUPATNAL ILLNESS NOT DEG 1-6</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>57.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PERM TOT OR PERM PRTL DISABLTY</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>18.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "DEGREE_INJURY_CD                    0     1    10     2  ...     7     8    9     ?\n",
              "DEGREE_INJURY                                            ...                       \n",
              "ACCIDENT ONLY                   219.0   NaN   NaN   NaN  ...   NaN   NaN  NaN   NaN\n",
              "ALL OTHER CASES (INCL 1ST AID)    NaN   NaN  20.0   NaN  ...   NaN   NaN  NaN   NaN\n",
              "DAYS AWAY FROM WORK ONLY          NaN   NaN   NaN   NaN  ...   NaN   NaN  NaN   NaN\n",
              "DAYS RESTRICTED ACTIVITY ONLY     NaN   NaN   NaN   NaN  ...   NaN   NaN  NaN   NaN\n",
              "DYS AWY FRM WRK & RESTRCTD ACT    NaN   NaN   NaN   NaN  ...   NaN   NaN  NaN   NaN\n",
              "FATALITY                          NaN  11.0   NaN   NaN  ...   NaN   NaN  NaN   NaN\n",
              "INJURIES DUE TO NATURAL CAUSES    NaN   NaN   NaN   NaN  ...   NaN  10.0  NaN   NaN\n",
              "INJURIES INVOLVNG NONEMPLOYEES    NaN   NaN   NaN   NaN  ...   NaN   NaN  3.0   NaN\n",
              "NO DYS AWY FRM WRK,NO RSTR ACT    NaN   NaN   NaN   NaN  ...   NaN   NaN  NaN   NaN\n",
              "NO VALUE FOUND                    NaN   NaN   NaN   NaN  ...   NaN   NaN  NaN  11.0\n",
              "OCCUPATNAL ILLNESS NOT DEG 1-6    NaN   NaN   NaN   NaN  ...  57.0   NaN  NaN   NaN\n",
              "PERM TOT OR PERM PRTL DISABLTY    NaN   NaN   NaN  18.0  ...   NaN   NaN  NaN   NaN\n",
              "\n",
              "[12 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 166
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 45.8 ms (started: 2021-10-25 13:19:51 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WTberi5145W"
      },
      "source": [
        "As 'ACCIDENT ONLY' doesn't fit any senarios, we checked their corresponding NARRATIVE contexts below to make the decision.\n",
        "<br>\n",
        "From the samples below, we find that most of the ACCIDENT ONLY are related to accident report and light injured degree, so it should be classified as 'light'. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wun2pmQoqSbx",
        "outputId": "33262f36-81c9-49de-c727-b1d7170fa128"
      },
      "source": [
        "# Look over ACCIDENT ONLY context in NARRATIVE.\n",
        "cnt = 5\n",
        "for i in range(len(df_org.DEGREE_INJURY)):\n",
        "    if df_org.DEGREE_INJURY[i] == 'ACCIDENT ONLY':\n",
        "        print(df_org.NARRATIVE[i],'\\n')\n",
        "        cnt-=1\n",
        "    if cnt ==1:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A ROOF FALL HAS OCCURRED IN THE L-5 ROOMS IN THE #2 (TRACK ENTRY) INBY SPAD #5+20. THE FALL IS IN AN INTERSECTION AND IS APPROX. 45' LONG, 18' WIDE & 8-10' HIGH. \n",
            "\n",
            "A roof fall occurred in the intersection of the #2 entry of J-main #2 at spad 41+05. The fall measured approx. 100' x 20' x 6'. The fall did extend beyond the horizon of the anchor bolts. \n",
            "\n",
            "The Cedar Creek elevator was out of service from 3:15 pm to 3:58 pm on 5/24/07 due to a door malfunction. \n",
            "\n",
            "The Cedar Creek Elevator was out of service from 6:53 am until 9:46 am due to an auxilary brake contactor fault. \n",
            "\n",
            "time: 11.2 ms (started: 2021-10-25 13:19:51 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5e7027c"
      },
      "source": [
        "#### 1.2.2 Decide two classes "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6BwgsEMpQCS"
      },
      "source": [
        "1. Based on the principal of class balance, we classify 'DEGREE_INJURY' as severe where 'DEGREE_INJURY_CD' is 1-5, and light where 'DEGREE_INJURY_CD' is 0 & 6-10.\n",
        "2. From the bar-chart below, we can see that the two classes are slightly imbalanced.\n",
        "3. There are 1128 instances (56.7%) in 'severe' class and 861 instances (43.3%) in 'light' class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4331fc87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aecb710-0bd3-4f83-b34d-d2372aa9fbde"
      },
      "source": [
        "# A function to assign to two different classes\n",
        "def category(row):\n",
        "    if row['DEGREE_INJURY_CD'] in ['1','2','3','4','5']:\n",
        "        val = \"severe\"\n",
        "    else:\n",
        "        val = \"light\"\n",
        "    return val"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4.73 ms (started: 2021-10-25 13:19:51 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58d8fca7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d54de6-3354-4a66-c51a-a4be24c3c3fe"
      },
      "source": [
        "# create 'classes' column based on the logic above\n",
        "df['classes'] = df.apply(category, axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 33.2 ms (started: 2021-10-25 13:19:51 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "325d94de",
        "outputId": "2fe3b86e-918c-402c-e44a-377e3035b14c"
      },
      "source": [
        "df['classes'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "severe    1128\n",
              "light      861\n",
              "Name: classes, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 170
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 10.1 ms (started: 2021-10-25 13:19:51 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtSEPTFnaSJ8",
        "outputId": "ae675afa-39e1-4160-9a66-7d723746c407"
      },
      "source": [
        "figure(figsize=(8, 8), dpi=80)\n",
        "# df[\"classes\"].hist()\n",
        "plt.hist(df[\"classes\"], 5, density= False)\n",
        "plt.title('Class Balance of Injury Degree')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAIVCAYAAAADCf8BAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAMTQAADE0B0s6tTgAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAcO0lEQVR4nO3df7ymdV3n8fcHB/DH8MMMBPnhmIAZ60oFaz9MMdTc2sdmkmVBCpVhP6wV3ZXI9rHt2ga1sda2iugmCmpFabZmZrmGZZj4C5VagdURBgUTQxh7gBLf/eO+Dt4dz8ycmXMPnxnO8/l43I855/p+r+v+nnsG7tdc13XO1BgjAABd9uleAACwvokRAKCVGAEAWokRAKCVGAEAWokRAKCVGAEAWokR2IaqOqOqtnSvY1dV1aiqJ3evYy2q6rSqur6qtlbVTy3omFdX1XMWcSxgMcQI61ZVPaaqfqeqPjO92W2uqjdW1Tc1r+uMqrp7WtPWqvp8Vb2ze133tqrakOSiJC8cY2wcY7x8hTknT9G1YbXHHWMcP8Z47SLXuhrTn687qur2qvpCVX2iqt5QVd9+b68F9jRihHWpqk5O8r4kNyf51iQHJDkhyZ8leWbfyu7x6ekNeGOShyW5Kslbq6qa13VvOizJA5N8qHsh86pqvzXs/jNjjAPGGAcleWKSq5O8q6p+YjGrW1lV7bs7jw9rJUZYr16Z5PfHGD83xtg8Zm4dY/z2GOPnV9qhqp5ZVR+oqn+oqs9V1R9V1SPmxh9bVZdX1a3TnA9U1aOmsSdV1funvxHfUlXvqaoHr2ahY4w7krw2yeFJvnY63uFV9daqunn6m/ZHqmqbEbWa+dMZhudPa9taVR+tqscvm/Ocqvrw9HXcXFW/MTf29XPPcWNVvbyqHrSdNd2/qs6vqk9Or9dfVtXjprEnJ/n4NPWqaT1H7+i1Wrq0VlVnTWcivlBVl1XVgXNzNlfVj08ff9WZleWX56rq4qr6vap6RVX9fZK3VNUlVfWaZc99YlXdWVWH7midSTLGuGGM8ctJzk/y60trrKr7VdULq+rvpvV/oKpOmXueqqpzanb56taqevW0vouXfY2/VFVvr6rbk7xw2v7sqrpqOu7VVfWsZV/D46rqL6Y/o5+qqv+yM2edYJeNMTw81tUjybFJRpKn7GDeGUm2zH3+tCSPTXK/zKLgfye5Ym78PUn+Y5IN0+OEJA+dxm5McmaSSrJfZmdjHrTK531Qkpcl+dskNW07MskzkmxMsm+SH0vy5STHz+03kjx5J+dfleSYaf0vS/KpufEfT/L3SZ48jR+Q5InT2NdOYy9Isv/0+Z8nedV2Xt//keSj0/Ptl9kb5u1JjpzGN01rOmY7xzh5mrNh7rW7K8l/T/KAzALu2iS/NLfP5iQ/vtL+23j9L55eqx+dXrsHJvm2JF9MctDcvFcl+Z3trPWe5122/VHTGr5r+vw/JfnwtH2fJN83Pdcjp/FnJ7klyeOm34el38uLlz3XTZn9OatpzWckuT7JidNxH5/ktiSPn1vH1iTPmo778OnPwy90/zfrcd9/tC/Aw+PefiT59ul//o/ewbx/9qa0wvg3Tsc5YPr8XUlevfSmsWzuJ5O8NMkRq1jfGUnuTnLr9Lh7evM5ZQf7XZXk+XOf3xMjOzH/2XOfHz9tWwqqjyV50TaOdXbmwmzudb4zyf1WmL9Pkn9M8r0rrOmc6eNN2bUYuSPJvnNzfi3Jn8x9fk8ULN9/pd/3zGLkihWe+8NJfnr6+MDpjfzk7az1nuddtv0B0xp+ePr8C5nCZG7OnyV5yfTxnyf51WXj789Xx8ivLJvzkSRnLdv2qiSvnj7+zSRvXDZ+WpLr1vLfm4fHah4u07AefXb69cid2amqnlizG0k/U1W3Jbl8Glo6LX9GZm8q/2e6VPCyqto4jf3bJF+X5ANVdd10Cn17p78/PcY4eIxxcGZnGn42yR9X1WOmtTy4ql41XeK4rapuzSweVrxEsBPzPz338RenXw+Yfn1EvnLpZLljk3zzdNng1un4b5tej8NWmP+1mb0J/79l269LssPLMTvwuTHGl+c+/2K+8jXsqk+usO0VSZbu9Tgts4D5i1049tLXe0tVPTSzsLls2Wv5bUmOmOYdkeRTy46xeRVrPjazy0Hzx/2hzO5JWhr/vmXjr8jKv3+wUGKEdWeMcW2Sa5L8yGr3qdlNi29N8vYkx40xDszsBsRkdho8Y4xPjTGeO8Z4eGZ/435Kkp+fxj46xvjhMcZhSb4/yfMyu2yzmvV+eYzx+sz+5v3UafN5Sb5+WsNBU7RcvbSWFezs/JVsTnLcNsZuSvJXSwE1PQ4aY9x/jHHjCvM/l9kZjEcu2/7IzC4l3Ftun36dv7flYSvMu3uFba9PsqmqviXJWZndh7QrTsssmK7I7EzYHUn+zbLX8kFjjJ+c5t+Y2SWUecs/X2nNNyX5qWXH3TjG+O658TcsGz9wzG6iht1KjLBenZXkmVV1QVU9fLop8MDpBr9fXmH+fpn9Tf4fxhi3V9XDMrvsco/pxscjq6oyuxZ/V5K7qmq/qjqzqg6Zpn4hyT9N4zs03dD4rCQPyezSQJIclNlljluS7FtVz8/sTMe27Oz8lfxGkhdX1XdOazqgqpaC7DVJvrGqfqqqHji9nkdV1dNXOtAY4+4kv53kP1fV102v0Qsyu3/k9Tu5rrW4JrMgOauq9qmqE/KVsx3bNcbYmuSSJP8zs/stdurbhavqiKo6J8k5Sf79GOO2McadSS5M8qtV9ejpdXxAVT2hqpZC8JIkP1pVJ1XVhqo6M7P7k3bkZUl+cdpvn6raf/r4m6fxlyf5/prdqL3f9Ht8TFU9bWe+LtgVYoR1aTqd/rjMTnm/L7M3pI9kdpPq768wf2tmN3C+pKq2JvmTJJctm/ak6VhbM7v34YrMvlMimZ0NubqqvpjZ5Z2Ls/03r4fV9HNGkvxDknOT/NgY453T+Esyi6ObMztj8dDMbqDdlp2d/1XGGBdldqbnZdOark3y9Gns+sxulnxKZpdebk3yp0kes51DvijJOzK71+azSU7N7KbiG3ZmXWsxxrg9yXMyC5DbkvxKZj/bZLUuTPJNSS4bY3x+FfN/q2bfzXRbZq//CZnd1/OKuTkvSvLGzP583ZrZ79fPZ3bzbJK8LrMbdN+U2Rmmx2d21u6O7T3xGOM3Mrs59sIkn8/sDMuvZTorNMa4MrPfv+dOY7dk9t/CSmddYKGW7swHWBeq6obMvkPkdQs41sGZXd44ZYyxU3G3SFX14SS/O8b4la41wFo4MwKsG1V1ZGZnha5dwLHul+TFSa66t0Okqn5wunxz/+ny1jfkq8/UwV5DjADrQlWdltnPNblojHHFGo/1mMwu65yaVd5jsmDPzeyMzN8nOT2zb5G+rmEdsBAu0wAArZwZAQBaiREAoNVe9w8g7b///uOQQw7Z8UQAYI9x4403fmmMsf9KY3tdjBxyyCHZsmXLjicCAHuMmv2r1ytymQYAaCVGAIBWYgQAaCVGAIBWYgQAaCVGAIBWYgQAaCVGAIBWYgQAaCVGAIBWYgQAaCVGAIBWYgQAaCVGAIBWYgQAaCVGAIBWYgQAaCVGAIBWYgQAaCVGAIBWYgQAaLWhewF7ik3n/HH3EtaNzed9T/cSANiDODMCALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAq1XFSFX9ZlVtrqpRVSfMbT+2qv66qq6pqiur6vi1jgEA68tqz4z8fpLHJ/nUsu2vTHLRGOO4JOcnuXgBYwDAOrKqGBljvHuMsWV+W1UdmuTEJJdOm/4gyVFVdcyujq3tSwEA9kZruWfkqCSfGWPclSRjjJHk+iRHr2Hsq1TV2VW1ZemxdevWNSwZANjT7PE3sI4xLhhjHLn02LhxY/eSAIAF2rCGfW9IcnhVbRhj3FVVldnZjeuT3LaLYwDAOrPLZ0bGGJ9N8sEkp0+bTk2yZYxx3a6O7epaAIC916rOjFTVK5N8T5LDkvxpVd0+xjgmyVlJLq6qczM743Hm3G67OgYArCOripExxlnb2P7xJN+6yDEAYH3Z429gBQDu28QIANBKjAAArcQIANBKjAAArcQIANBKjAAArcQIANBKjAAArcQIANBKjAAArcQIANBKjAAArcQIANBKjAAArcQIANBKjAAArcQIANBKjAAArcQIANBKjAAArcQIANBKjAAArcQIANBKjAAArcQIANBKjAAArcQIANBKjAAArcQIANBKjAAArcQIANBKjAAArcQIANBKjAAArcQIANBKjAAArcQIANBKjAAArcQIANBKjAAArcQIANBKjAAArcQIANBKjAAArcQIANBKjAAArcQIANBKjAAArcQIANBKjAAArTZ0LwAAdmTTOX/cvYR1YfN539PyvM6MAACtxAgA0EqMAACtxAgA0EqMAACtxAgA0EqMAACtxAgA0EqMAACtxAgA0EqMAACtxAgA0EqMAACtxAgA0EqMAACtxAgA0EqMAACtxAgA0EqMAACtxAgA0GohMVJV311VH6yqD1fVx6rqOdP2Q6vq7VV17bT9CXP7bHMMAFg/Nqz1AFVVSS5NcvIY4yNVtSnJ/62qNyU5L8l7xxhPq6qTkry5qh4xxvjyDsYAgHViUZdpRpKDp48PTHJLkjuT/ECSC5NkjHFlkk8neeI0b3tjAMA6seYzI2OMUVU/mORNVfXFJA9O8owkByTZd4xx09z0zUmOrqqHbGtsresBAPYuaz4zUlUbkrwkyTPGGA9PckqSS7KA0JmOf3ZVbVl6bN26dRGHBQD2EIu4THNCkoeNMd6d3HPJZUuSf5nkrqo6bG7upiTXjzFu2dbY8oOPMS4YYxy59Ni4ceMClgwA7CkWESM3JDm8qh6dJFV1TJJHJvl4ksuSPG/aflKSI5JcPu23vTEAYJ1YxD0jN1fVTyT5vaq6O7PA+ZkxxvVV9eIkl1TVtUm+lOT0ue+W2d4YALBOLOS+jjHGG5O8cYXtNyd56jb22eYYALB++AmsAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAECrhcRIVe1fVb9VVddW1Uer6tJp+7FV9ddVdU1VXVlVx8/ts80xAGD9WNSZkfOSjCTHjTEek+RF0/ZXJrlojHFckvOTXDy3z/bGAIB1Ys0xUlUPSvJjSX5hjDGSZIxxU1UdmuTEJJdOU/8gyVFVdcz2xta6HgBg77KIMyOPTPL5JOdW1fur6i+r6pQkRyX5zBjjriSZQuX6JEfvYOyfqaqzq2rL0mPr1q0LWDIAsKdYRIxsSPLwJH87xjgxyc8m+d1p+5qNMS4YYxy59Ni4ceMiDgsA7CEWESPXJ7k7yeuTZIzxoSSfzCxQDq+qDUlSVZXZmY/rk9ywnTEAYB1Zc4yMMT6X5J1JvitJquoRSR6R5D1JPpjk9GnqqUm2jDGuG2N8dltja10PALB3WcillCTPS/K/qur8zM6SnDXGuLGqzkpycVWdm+S2JGfO7bO9MQBgnVjUfR2fSPKkFbZ/PMm3bmOfbY4BAOuHn8AKALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALQSIwBAKzECALRaWIxU1ZlVNarq6dPnh1bV26vq2qr6WFU9YW7uNscAgPVlITFSVZuSPDfJe+c2n5fkvWOMY5OcmeQNVbXvKsYAgHVkzTFSVfskeXWS5ye5c27oB5JcmCRjjCuTfDrJE1cxBgCsI4s4M3J2kveMMT6wtKGqHpJk3zHGTXPzNic5entjKx28qs6uqi1Lj61bty5gyQDAnmJNMVJV/yLJqUleupjlfLUxxgVjjCOXHhs3btxdTwUANFjrmZHvSLIpybVVtTnJtyS5KLPLMHdV1WFzczcluX6Mccu2xta4FgBgL7SmGBljvGKMcfgYY9MYY1NmN7D+xBjjFUkuS/K8JKmqk5IckeTyadftjQEA68iG3XjsFye5pKquTfKlJKePMb68ijEAYB1ZaIyMMU6e+/jmJE/dxrxtjgEA64ufwAoAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAEArMQIAtBIjAECrNcdIVd2/qv6wqq6pqquq6s+q6php7NCqentVXVtVH6uqJ8ztt80xAGD9WNSZkYuSPGqM8dgkb0ny6mn7eUneO8Y4NsmZSd5QVfuuYgwAWCfWHCNjjDvGGG8bY4xp03uTbJo+/oEkF07zrkzy6SRPXMUYALBO7I57Rn4uyVuq6iFJ9h1j3DQ3tjnJ0dsb2w3rAQD2YBsWebCqOjfJMUlOSfKABR3z7CRnL31+0EEHLeKwAMAeYmFnRqrqRUmekeRfjzH+cYxxS5K7quqwuWmbkly/vbHlxx1jXDDGOHLpsXHjxkUtGQDYAywkRqazFz+U5CljjFvnhi5L8rxpzklJjkhy+SrGAIB1Ys2XaarqyCS/nuQTSd5VVUly5xjjcUlenOSSqro2yZeSnD7G+PK06/bGAIB1Ys0xMsbYkqS2MXZzkqfu7BgAsH74CawAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQCsxAgC0EiMAQKvWGKmqY6vqr6vqmqq6sqqO71wPAHDv6z4z8sokF40xjktyfpKLe5cDANzb2mKkqg5NcmKSS6dNf5DkqKo6pmtNAMC9b0Pjcx+V5DNjjLuSZIwxqur6JEcnuW5pUlWdneTsuf3+qapu2g3r2Zhk6244LsvU+d0rAGAldf5ufS88ZFsDnTGyKmOMC5JcsLufp6q2jDGO3N3PAwB7qq73ws57Rm5IcnhVbUiSqqrMzopc37gmAOBe1hYjY4zPJvlgktOnTacm2TLGuG7bewEA9zXdl2nOSnJxVZ2b5LYkZzauZbdfCgKAPVzLe2GNMTqeFwAgSf/PGQEA1jkxAgC0EiMAQCsxAgD3YVU1qurg6eO3VdWjVrHPxVX177YxdnJVPW2Ra+z+bpq9UlVtWPrJsQCwtxhjfPcCDnNykoOTvH0Bx0pyHzgzUlUPqKrfraq/raqrquod0/Yfqaq/qaoPVtW7q+qx0/ZrqurEuf3PqKo3Tx8fVlW/V1Xvq6qPVtVL5+Ztrqrzq+p9SV5bVftW1XnT3A9P+z34Xv7yAWDVpveyE6aPv76qrqiqq6vqTVX1jqo6Y276o6vqndP75puqar9p3+clOW167/uPi1jXfeHMyNOSHDzG+IYkqaqvqapvT/JDSZ4wxrizqr4jyRuSHJ/Zvwx8RpL3T/ufmeS/TR+/Nsl/HWNcPv1k2LdW1TPHGJdN4w9J8rjp39E5N8kXxxj/anreX0zy0iQ/vXu/XABYiEuSvHyM8ZqqenSSD2X2XrnkhCRPSnJnkncnOXWM8caqujCz990VL+PsivtCjFyVWb29PMnlSd6W5HuTPDbJ38x+ynyS5Guq6gFJXpfkQ1X1wiRHJDkuyZ9U1YOSnJLkoXP7bEwyf23t4vGVH8zy9CQHVdWp0+f7Jdm8+C8PABarqg7MLDZelyRjjL+rqr9aNu3NY4x/nOa/L8kjd9d69voYGWN8oqq+Icl3Jnlykl9N8o4krx1jnLvCLluq6v2ZBcvxSS4dY9xVVfefxr9ljHHHNp5u/l8yrCTPH2O8YyFfCAD0Wv5TUOffC/8pu7EZ7gv3jByZZIwx/ijJizKLhEuTnF5VR09z9pm/TyTJa5L8aJJnJ/ntzA6wNcm7kpwzd+yHTcdfyR8meUFVPXCa+8CqOn6hXxwA7AZjjNsyu7JwepJM32Hz+FXufluSgxa5nr0+RpI8Jsl7quqqzK53XTLGuDzJf0jy5mn71UmeNbfPW5KclOTmMcbfzW0/LckxVfWxqvpokjdldp/ISs5PcmVml4I+kuS9mZ3yAoC9wbOT/GRVfSxfeU+7dRX7vTnJCYu8gdW/TQMA61BVbczsGzFGVT0iyRVJThpj3HBvr2Wvv2cEANgl35bk16Zv2rhfkhd0hEjizAgA0Oy+cM8IALAXEyMAQCsxAgC0EiMAQCsxAgC0EiMAQKv/D4/OtkPb4X5TAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 640x640 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 145 ms (started: 2021-10-25 13:19:51 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZdpJfLhnd7x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7cd8bc5-5946-43a8-c98d-6eabc1f92480"
      },
      "source": [
        "df.classes.value_counts()/len(df.classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "severe    0.567119\n",
              "light     0.432881\n",
              "Name: classes, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 172
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 10.3 ms (started: 2021-10-25 13:19:52 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9L5Tun25xpY"
      },
      "source": [
        "The dataset looks like below after the initial cleaning up and class determination."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a49cc53",
        "outputId": "01cb34f3-deb6-4e81-b525-0a74392521d1"
      },
      "source": [
        "# the table with narrative and the corresponding classes\n",
        "df = df[['classes','NARRATIVE']]\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>classes</th>\n",
              "      <th>NARRATIVE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>severe</td>\n",
              "      <td>Employee was cleaning up at the Primary Crushe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>light</td>\n",
              "      <td>Handle of sledgehammer broke and head of hamme...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>severe</td>\n",
              "      <td>EMPLOYEE WAS CLIMBING DOWN A LADDER AND WHEN H...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>severe</td>\n",
              "      <td>HE PULLED A BACK MUSCLE WHILE STACKING BAGS OF...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>severe</td>\n",
              "      <td>EE hands began to break out in a rash after he...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  classes                                          NARRATIVE\n",
              "0  severe  Employee was cleaning up at the Primary Crushe...\n",
              "1   light  Handle of sledgehammer broke and head of hamme...\n",
              "2  severe  EMPLOYEE WAS CLIMBING DOWN A LADDER AND WHEN H...\n",
              "3  severe  HE PULLED A BACK MUSCLE WHILE STACKING BAGS OF...\n",
              "4  severe  EE hands began to break out in a rash after he..."
            ]
          },
          "metadata": {},
          "execution_count": 173
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 20.7 ms (started: 2021-10-25 13:19:52 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9e2915b"
      },
      "source": [
        "## 2 Data Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e4c48fd"
      },
      "source": [
        "The pre-processing follows the precedures listed below:\n",
        "1. Tokenize\n",
        "2. Lemmatize/Stemming\n",
        "3. Remove stop words based on TF/IDF - create the Dataset for each task\n",
        "4. ??Create two Vectorizers for each task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a4e137b"
      },
      "source": [
        "### 2.1 Tokenize\n",
        "Clean sentence with removing punctuations and lower case. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d44192eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4f0dc7a-126e-4cf3-80d9-0506f24a5c0b"
      },
      "source": [
        "# Clean sentences\n",
        "# Input string, output string\n",
        "def remove_punc(text):\n",
        "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
        "    text = re.sub(r'[^\\w\\s]','',text) # remove punctuation which is connected to words\n",
        "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.94 ms (started: 2021-10-25 13:19:52 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AR7Y-KnOdGt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a539b307-b823-4373-8d82-9be6823bd2f5"
      },
      "source": [
        "def clean_text(sentences):\n",
        "    cleaned_sentences = [] # list of sentence, with every character of sentence.\n",
        "    cleaned_sen_words = [] # list of lists of sentence, with every string of words.\n",
        "    for sentence in sentences:\n",
        "        cleaned_sentences.append(remove_punc(sentence))\n",
        "    for sentence in cleaned_sentences:\n",
        "        cleaned_sen_words.append(list(filter(None, sentence.split(' ')))) # Remove empty string of list\n",
        "\n",
        "    return cleaned_sen_words\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 6.5 ms (started: 2021-10-25 13:19:52 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfQpIXR7dstK"
      },
      "source": [
        "#### Tokenize words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c08da9fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecd5004d-6653-4eec-d6af-6d9bd79d26b6"
      },
      "source": [
        "# Tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "def tokenize(sentences):\n",
        "    tokenized_words = []\n",
        "    tokenized_sentences = []\n",
        "    for sentence in sentences:\n",
        "        temp_words = []\n",
        "        for word in sentence:\n",
        "            temp_words.append(' '.join(map(str, word_tokenize(word))))\n",
        "            tokenized_words.append(' '.join(map(str, word_tokenize(word))))\n",
        "        tokenized_sentences.append(temp_words)\n",
        "    return tokenized_words, tokenized_sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 8.77 ms (started: 2021-10-25 13:19:52 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "216d99a5"
      },
      "source": [
        "### 2.2 Lemmatize and compare with stemming\n",
        "Below lists the original sentence, the sentence after stemming and lemmatizing. We can see that sentence after Lemmatizing offers better form of words. Therefore, we chose Lemmatizing to preprocess our data.\n",
        "\n",
        "**Original Sentence:** employee was cleaning up at the primary crusher with the dingo skid steer the employee slipped and fell while operating the skid steer and the machine pinned him against the cement retaining wall\n",
        "\n",
        "**Stemming:** employe wa clean up at the primari crusher with the dingo skid steer the employe slip and fell while oper the skid steer and the machin pin him against the cement retain wall\n",
        "\n",
        "**Lemmatize:** employee be clean up at the primary crusher with the dingo skid steer the employee slip and fell while operate the skid steer and the machine pin him against the cement retain wall\n",
        "\n",
        "1. Stemming: refers to reducing a word to its root form.\n",
        "<br>\n",
        "Stemming:4552, Stemming(remove duplicated words): 3369 \n",
        "2. Lemmatize: converts words in the second or third forms to their first form variants, so they are still dictionary words.\n",
        "<br>\n",
        "Lemmatize:4552, Lemmatize(remove duplicated words): 4235 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57ca912b"
      },
      "source": [
        "#### 2.2.1 Stemming\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16tOTtxUnPOW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "783c4034-8ae7-424c-8613-cf665a72b1ec"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "def stem(sentences):\n",
        "    stem_words = []\n",
        "    stem_sentences = []\n",
        "    for sentence in tokenized_sentences:\n",
        "        temp_words = []\n",
        "        for word in sentence:\n",
        "            temp_words.append(''.join(map(str,ps.stem(word))))\n",
        "            stem_words.append(''.join(map(str,ps.stem(word))))\n",
        "        stem_sentences.append(temp_words)\n",
        "    return stem_words, stem_sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 8.79 ms (started: 2021-10-25 13:19:52 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "363f6c07"
      },
      "source": [
        "#### 2.2.2 Lemmatize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrQF0-xl8kYX"
      },
      "source": [
        "Different from stemming, lemmatization requires us to access the part of speech tags (pos_tag), and restore words from different forms. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piXvVJoYBXKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e09b5a38-8963-4ed6-a727-c4b3a6f1846c"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "word_lem = WordNetLemmatizer()\n",
        "\n",
        "# Fetch the pos_tag function\n",
        "def fetch_pos_tag(pos_tag):\n",
        "    if pos_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif pos_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif pos_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    elif pos_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def lemma(sentences):\n",
        "    lemma_words = []\n",
        "    lemma_sentences = []\n",
        "    for sentence in sentences:\n",
        "        tagged_sent = (nltk.pos_tag(sentence)) # [('employee','NN'),('was', 'VBD'),('foreign', 'JJ')...]\n",
        "        lemma_sentence = []\n",
        "        for tag in tagged_sent:\n",
        "            wordnet_pos = fetch_pos_tag(tag[1]) or wordnet.NOUN # if None then regard as Noun # n,v,a,r\n",
        "            lemma_words.append(word_lem.lemmatize(tag[0], pos = wordnet_pos))\n",
        "            lemma_sentence.append(word_lem.lemmatize(tag[0], pos = wordnet_pos))\n",
        "        lemma_sentences.append(lemma_sentence)\n",
        "    return lemma_words, lemma_sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 17.1 ms (started: 2021-10-25 13:19:52 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bf0dbf4"
      },
      "source": [
        "### 2.3 Remove stop words (rank words according to TF/IDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ_wTYqXByrF"
      },
      "source": [
        "#### TfidfVectorizer\n",
        "For removing stop words, we used TfidfVectorizer in scikit-learn. The stop words in the narrative text were removed according to TF/IDF ranking."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAgib6HQoazq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f452e291-ea42-460b-ba7e-db1a32cf1dcc"
      },
      "source": [
        "# Preprocessing\n",
        "# Convert list of lists of sentences into list of sentences\n",
        "# Example: [['Employee be drive'],['ee'],...['be here']] --> ['Employee be drive','ee',...'be here']\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "def remove_stop(sentences):\n",
        "    tfidf_sentences = []\n",
        "    for sentence in sentences:\n",
        "        word_list = []\n",
        "        for word in sentence:\n",
        "            word_list.append(word)\n",
        "        tfidf_sentences.append(' '.join(word_list))\n",
        "\n",
        "    tfidf_vectorizer = TfidfVectorizer(stop_words='english', min_df =0, use_idf = True) # TF-IDF vectorizer with removing 'English' stop words\n",
        "    tfidf_vectors = tfidf_vectorizer.fit_transform(tfidf_sentences) # Transform sentences into vectors\n",
        "    tfidf_array = tfidf_vectorizer.fit_transform(tfidf_sentences).toarray() # Transform vectors into arrays\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(tfidf_sentences).todense() # Sent all the arrays as one matrix\n",
        "    feature_names = tfidf_vectorizer.get_feature_names()\n",
        "    vector_list = []\n",
        "\n",
        "    for matrix_index in range(len(tfidf_sentences)):\n",
        "\n",
        "        feature_index = tfidf_matrix[matrix_index,:].nonzero()[1]\n",
        "        tfidf_scores = zip([feature_names[i] for i in feature_index], [tfidf_matrix[matrix_index, x] for x in feature_index]) # example: tfidf_scores = ('ee',0.32132),('cement',0.1231)\n",
        "        vector_list.append(dict(tfidf_scores)) # 1989 rows dictionaries with words and vector\n",
        "        \n",
        "    # Return terms per document with nonzero entries in array (each sentence)\n",
        "    inverse_array = tfidf_vectorizer.inverse_transform(tfidf_array)\n",
        "    filtered_sentences = []\n",
        "    for rows in range(len(sentences)): # 1989 rows\n",
        "        filtered_sentence = []\n",
        "        for word in sentences[rows]:\n",
        "            if word in inverse_array[rows]:\n",
        "                filtered_sentence.append(word)\n",
        "        filtered_sentences.append(filtered_sentence)\n",
        "    return filtered_sentences, vector_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 22.8 ms (started: 2021-10-25 13:19:52 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy9nnfOpOugJ"
      },
      "source": [
        "Put all functions together "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6KNStkSOtnk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "817d92b8-582a-47e1-ca9d-0f10dad91e69"
      },
      "source": [
        "# Input NARRATIVE sentences, output preprocessed sentences\n",
        "# Preprocessing includes Tokenization, Lemmatization and Removing stop words\n",
        "def preprocessing(sentences, lemma_or_stem = 'lemma'):\n",
        "    # Remove punctuations and lower cases\n",
        "    cleaned_sen_words = clean_text(sentences)\n",
        "    tokenized_words, tokenized_sentences = tokenize(cleaned_sen_words)\n",
        "    if lemma_or_stem == 'lemma':\n",
        "        lemma_words, lemma_sentences = lemma(tokenized_sentences)\n",
        "        filtered_sentences, filtered_vector = remove_stop(lemma_sentences)\n",
        "    else:\n",
        "        stem_words, stem_sentences = stem(tokenized_sentences)\n",
        "        filtered_sentences, filtered_vector = remove_stop(stem_sentences)\n",
        "    # Convert list to string\n",
        "    filtered_string = []\n",
        "    for index in range(len(filtered_sentences)): # 1989\n",
        "        string = ''\n",
        "        for word in filtered_sentences[index]:\n",
        "            string = string + word + ' '\n",
        "        filtered_string.append(string)\n",
        "    return filtered_string, filtered_vector"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 12.3 ms (started: 2021-10-25 13:19:52 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoREB2gzmeSN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ce334c0-ed31-4ecc-b3e6-8b5b6ef8efeb"
      },
      "source": [
        "filtered_column, filtered_vector = preprocessing(df['NARRATIVE'],'lemma')\n",
        "# within column 'narrative' is the narrative after tokenization, lemmatisation and removing stop words\n",
        "df['narrative'] = filtered_column\n",
        "df['filtered_NARRATIVE_vector'] = filtered_vector"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 13.9 s (started: 2021-10-25 13:19:52 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dfa36d1"
      },
      "source": [
        "## 3 Split the training/testing/validation dataset\n",
        "1. In this section, we prepare the training, testing and validation dataset on a 70%,15%,15% proportion.\n",
        "2. There are 1386 instances in training set, 306 in testing set and 297 in validation set.\n",
        "3. Then we assign 'train','test','val' to each row and write the split dataset back to file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd3ba0f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be49ad85-c696-4f3b-8c5e-336e5b9d0f51"
      },
      "source": [
        "args = Namespace(\n",
        "    window_size=5,\n",
        "    train_proportion=0.7,\n",
        "    val_proportion=0.15,\n",
        "    test_proportion=0.15,\n",
        "    output_munged_csv=\"../NLP/proj2_with_splits_task1.csv\",\n",
        "    seed=1337\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.62 ms (started: 2021-10-25 13:20:06 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fb75bc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33f61c3e-69d6-4004-d19f-96b1af1f4243"
      },
      "source": [
        "# Create split data\n",
        "n = len(df)\n",
        "\n",
        "def get_split(row_num):\n",
        "    if row_num <= n*args.train_proportion:\n",
        "        return 'train'\n",
        "    elif (row_num > n*args.train_proportion) and (row_num <= n*args.train_proportion + n*args.val_proportion):\n",
        "        return 'val'\n",
        "    else:\n",
        "        return 'test'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4.59 ms (started: 2021-10-25 13:20:06 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d91662ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ee46ed4-ccb8-4b7e-d150-6aa760cb4d0a"
      },
      "source": [
        "df['split']= df.apply(lambda row: get_split(row.name), axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 22.8 ms (started: 2021-10-25 13:20:06 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sru2yn0km9xT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "016ab920-d37a-4fcf-9d17-c2d48f091f40"
      },
      "source": [
        "# get the needed columns for further modeling\n",
        "doc = df.copy() [['classes','narrative','split']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.77 ms (started: 2021-10-25 13:20:06 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7zsQFftAwHi"
      },
      "source": [
        "The dataset looks like below after the data preprocessing and train/test/validaion splitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "l2O75dHbm9lM",
        "outputId": "33956ab0-50f3-4224-9a0f-e130f1bad9ad"
      },
      "source": [
        "doc.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>classes</th>\n",
              "      <th>narrative</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>severe</td>\n",
              "      <td>employee clean primary crusher dingo skid stee...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>light</td>\n",
              "      <td>handle sledgehammer broke head hammer hit empl...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>severe</td>\n",
              "      <td>employee climb ladder step ground slip sprain ...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>severe</td>\n",
              "      <td>pull muscle stack bag material</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>severe</td>\n",
              "      <td>ee hand begin break rash handle material coat ...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  classes                                          narrative  split\n",
              "0  severe  employee clean primary crusher dingo skid stee...  train\n",
              "1   light  handle sledgehammer broke head hammer hit empl...  train\n",
              "2  severe  employee climb ladder step ground slip sprain ...  train\n",
              "3  severe                    pull muscle stack bag material   train\n",
              "4  severe  ee hand begin break rash handle material coat ...  train"
            ]
          },
          "metadata": {},
          "execution_count": 186
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 19.6 ms (started: 2021-10-25 13:20:06 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aK8N5oxisb3L",
        "outputId": "dae6af0d-5f11-4101-ba8c-3b2476202415"
      },
      "source": [
        "# look at the number of instances for training, testing and validation set\n",
        "doc['split'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "train    1386\n",
              "test      306\n",
              "val       297\n",
              "Name: split, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 187
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 8.15 ms (started: 2021-10-25 13:20:06 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXVvvS2jtkF1"
      },
      "source": [
        "Write the dataset back to csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ae687fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d055c912-b05c-4122-a71e-9e8c93b6c9da"
      },
      "source": [
        "# Write the dataset back to file\n",
        "doc.to_csv(args.output_munged_csv, index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 26.5 ms (started: 2021-10-25 13:20:06 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c247a506"
      },
      "source": [
        "## 4 Feed-forward Neural Network model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsr59IJdUcvr"
      },
      "source": [
        "## One-Hot-Encoding\n",
        "1. For this task, we implemented one-hot-encoding as our word embedding method. It is a basic method of converting the text data into numerical vectors so that we could train our model on it.\n",
        "\n",
        "2. With one-hot-encoding, each unique word in vocabulary was represented with value 1 and the rest with 0. For each word, there is a a one-hot encoded vector, in which 1 represents the position of the word in the sentence and 0 for other words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7m76iJ_nytln"
      },
      "source": [
        "### 4.1 Converting Text Inputs to Vectorized Minibatches\n",
        "\n",
        "The three classes presented in this notebook are responsible for \n",
        "- mapping each token to an integer to create a  `Vocabulary`, \n",
        "- applying this mapping to each data point to create a vectorized form using `Vectorizer`, and \n",
        "- grouping the vectorized data points into a minibatch for the model through `DataLoader`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoFshHIfwEW6"
      },
      "source": [
        "#### 4.1.1 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glH5eHjRwEW8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db1e23fd-a833-4993-f865-91d4acd9f67c"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, review_df, vectorizer):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        review_df (pandas.DataFrame): the dataset\n",
        "        vectorizer (ReviewVectorizer): vectorizer instantiated from dataset\n",
        "        \"\"\"\n",
        "        self.review_df = review_df\n",
        "        self._vectorizer = vectorizer\n",
        "        self.train_df = self.review_df[self.review_df.split=='train']\n",
        "        self.train_size = len(self.train_df)\n",
        "        self.val_df = self.review_df[self.review_df.split=='val']\n",
        "        self.validation_size = len(self.val_df)\n",
        "        self.test_df = self.review_df[self.review_df.split=='test']\n",
        "        self.test_size = len(self.test_df)\n",
        "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
        "                            'val': (self.val_df, self.validation_size),\n",
        "                            'test': (self.test_df, self.test_size)}\n",
        "        self.set_split('train')\n",
        "    @classmethod\n",
        "    def load_dataset_and_make_vectorizer(cls, review_csv):\n",
        "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
        "        Args:\n",
        "            review_csv (str): location of the dataset\n",
        "        Returns:\n",
        "            an instance of ReviewDataset\n",
        "        \"\"\"\n",
        "        review_df = pd.read_csv(review_csv)\n",
        "        return cls(review_df, ReviewVectorizer.from_dataframe(review_df))\n",
        "    def get_vectorizer(self):\n",
        "        \"\"\" returns the vectorizer \"\"\"\n",
        "        return self._vectorizer\n",
        "    def set_split(self, split=\"train\"):\n",
        "        \"\"\" selects the splits in the dataset using a column in the dataframe\n",
        "        Args:\n",
        "        split (str): one of \"train\", \"val\", or \"test\"\n",
        "        \"\"\"\n",
        "        self._target_split = split\n",
        "        self._target_df, self._target_size = self._lookup_dict[split]\n",
        "    def __len__(self):\n",
        "        return self._target_size\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"the primary entry point method for PyTorch datasets\n",
        "        Args:\n",
        "        index (int): the index to the data point\n",
        "        Returns:\n",
        "        a dict of the data point's features (x_data) and label (y_target)\n",
        "        \"\"\"\n",
        "        row = self._target_df.iloc[index]\n",
        "        review_vector = \\\n",
        "        self._vectorizer.vectorize(row.narrative)\n",
        "        rating_index = \\\n",
        "        self._vectorizer.rating_vocab.lookup_token(row.classes)\n",
        "        return {'x_data': review_vector,\n",
        "                'y_target': rating_index}\n",
        "    def get_num_batches(self, batch_size):\n",
        "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
        "        Args:\n",
        "        batch_size (int)\n",
        "        Returns:\n",
        "        number of batches in the dataset\n",
        "        \"\"\"\n",
        "        return len(self) // batch_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 35 ms (started: 2021-10-25 13:20:06 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffDhnuxrwEXB"
      },
      "source": [
        "#### 4.1.2 The `Vocabulary` Class\n",
        "\n",
        "The first stage to map each token to a numerical version of itself. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeaS_rZ8wEXE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f43d4270-d0fe-4701-e85a-707a8b55bf3e"
      },
      "source": [
        "class Vocabulary(object):\n",
        "    \"\"\"Class to process text and extract Vocabulary for mapping\"\"\"\n",
        "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            token_to_idx (dict): a pre-existingmap of tokens to indices\n",
        "            add_unk (bool): a flag that indicates whether to add the UNK token\n",
        "            unk_token (str): the UNK token to add into the Vocabulary\n",
        "        \"\"\"\n",
        "        if token_to_idx is None:\n",
        "            token_to_idx = {}\n",
        "        self._token_to_idx = token_to_idx\n",
        "        self._idx_to_token = {idx: token \n",
        "                                for token, idx in self._token_to_idx.items()}\n",
        "        self._add_unk = add_unk\n",
        "        self._unk_token = unk_token\n",
        "        self.unk_index = 1\n",
        "        if add_unk:\n",
        "            self.unk_index = self.add_token(unk_token)\n",
        "    def to_serializable(self):\n",
        "        \"\"\" returns a dictionary that can be serialized \"\"\"\n",
        "        return {'token_to_idx': self._token_to_idx,\n",
        "                'add_unk': self._add_unk,\n",
        "                'unk_token': self._unk_token}\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n",
        "        return cls(**contents)\n",
        "    def add_token(self, token):\n",
        "        \"\"\"Update mapping dicts based on the token.\n",
        "        Args:\n",
        "            token (str): the item to add into the Vocabulary\n",
        "        Returns:\n",
        "            index (int): the integer corresponding to the token\n",
        "        \"\"\"\n",
        "        if token in self._token_to_idx:\n",
        "            index = self._token_to_idx[token]\n",
        "        else:\n",
        "            index = len(self._token_to_idx)\n",
        "            self._token_to_idx[token] = index\n",
        "            self._idx_to_token[index] = token\n",
        "        return index\n",
        "    def lookup_token(self, token):\n",
        "        \"\"\"Retrieve the index associated with the token\n",
        "        or the UNK index if token isn't present.\n",
        "        Args:\n",
        "            token (str): the token to look up\n",
        "        Returns:\n",
        "            index (int): the index corresponding to the token\n",
        "        Notes:\n",
        "            `unk_index` needs to be >=0 (having been added into the Vocabulary)\n",
        "            for the UNK functionality\n",
        "        \"\"\"\n",
        "        if self._add_unk:\n",
        "            return self._token_to_idx.get(token, self.unk_index)\n",
        "        else:\n",
        "            return self._token_to_idx[token]\n",
        "    def lookup_index(self, index):\n",
        "        \"\"\"Return the token associated with the index\n",
        "        Args:\n",
        "            index (int): the index to look up\n",
        "        Returns:\n",
        "            token (str): the token corresponding to the index\n",
        "        Raises:\n",
        "        KeyError: if the index is not in the Vocabulary\n",
        "        \"\"\"\n",
        "        if index not in self._idx_to_token:\n",
        "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
        "        return self._idx_to_token[index]\n",
        "    def __str__(self):\n",
        "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
        "    def __len__(self):\n",
        "        return len(self._token_to_idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 34.4 ms (started: 2021-10-25 13:20:06 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5g5rGWCwEXH"
      },
      "source": [
        "#### 4.1.3 Vectorizer\n",
        "\n",
        "The second stage to convert a text dataset to a vectorized minibatch is to iterate through the tokens of an input data point and convert each token to its integer form. The result of this iteration should be a vector. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Hdmbk4dwEXJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a0c31b-6b90-40ec-9d98-f4d04cc54e2b"
      },
      "source": [
        "from collections import Counter\n",
        "import string\n",
        "class ReviewVectorizer(object):\n",
        "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"\n",
        "    def __init__(self, review_vocab, rating_vocab):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            review_vocab (Vocabulary): maps words to integers\n",
        "            rating_vocab (Vocabulary): maps class labels to integers\n",
        "        \"\"\"\n",
        "        self.review_vocab = review_vocab\n",
        "        self.rating_vocab = rating_vocab\n",
        "    def vectorize(self, review):\n",
        "        \"\"\"Create a collapsed one hot vector for the review\n",
        "        Args:\n",
        "            review (str): the review\n",
        "        Returns:\n",
        "            one_hot (np.ndarray): the collapsed onehot encoding\n",
        "        \"\"\"\n",
        "        one_hot = np.zeros(len(self.review_vocab), dtype=np.float32)\n",
        "        for token in review.split(\" \"):\n",
        "            if token not in string.punctuation:\n",
        "                one_hot[self.review_vocab.lookup_token(token)] = 1\n",
        "        return one_hot\n",
        "    @classmethod\n",
        "    def from_dataframe(cls, review_df, cutoff=25):\n",
        "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
        "        Args:\n",
        "            review_df (pandas.DataFrame): the review dataset\n",
        "            cutoff (int): the parameter for frequency based filtering\n",
        "        Returns:\n",
        "            an instance of the ReviewVectorizer\n",
        "        \"\"\"\n",
        "        review_vocab = Vocabulary(add_unk=True)\n",
        "        rating_vocab = Vocabulary(add_unk=False)\n",
        "        # Add classes\n",
        "        for classes in sorted(set(review_df.classes)):\n",
        "            rating_vocab.add_token(classes)\n",
        "        # Add top words if count > provided count\n",
        "        word_counts = Counter()\n",
        "        for narrative in review_df.narrative:\n",
        "            for word in narrative.split(\" \"):\n",
        "                if word not in string.punctuation:\n",
        "                    word_counts[word] += 1\n",
        "        for word, count in word_counts.items():\n",
        "            if count > cutoff:\n",
        "                review_vocab.add_token(word)\n",
        "        return cls(review_vocab, rating_vocab)\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        \"\"\"Intantiate a ReviewVectorizer from a serializable dictionary\n",
        "        Args:\n",
        "            contents (dict): the serializable dictionary\n",
        "        Returns:\n",
        "            an instance of the ReviewVectorizer class\n",
        "        \"\"\"\n",
        "        review_vocab = Vocabulary.from_serializable(contents['review_vocab'])\n",
        "        rating_vocab = Vocabulary.from_serializable(contents['rating_vocab'])\n",
        "        return cls(review_vocab=review_vocab, rating_vocab=rating_vocab)\n",
        "    def to_serializable(self):\n",
        "        \"\"\"Create the serializable dictionary for caching\n",
        "        Returns:\n",
        "            contents (dict): the serializable dictionary\n",
        "        \"\"\"\n",
        "        return {'review_vocab': self.review_vocab.to_serializable(),\n",
        "                'rating_vocab': self.rating_vocab.to_serializable()}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 32.8 ms (started: 2021-10-25 13:20:06 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fn8ADkI4eQ_3"
      },
      "source": [
        "#### 4.1.4 Dataloader\n",
        "\n",
        "The final stage of the text to vectorized minibatch pipeline is to actually group the vectorized data\n",
        "points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqcdM04aeQ_6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "881c6d16-57fb-471f-a11e-3bae9afb4aa5"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    A generator function which wraps the PyTorch DataLoader. It will\n",
        "    ensure each tensor is on the write device location.\n",
        "    \"\"\"\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, \n",
        "                            shuffle=shuffle, drop_last=drop_last)\n",
        "    for data_dict in dataloader:\n",
        "        out_data_dict = {}\n",
        "        for name, tensor in data_dict.items():\n",
        "            out_data_dict[name] = data_dict[name].to(device)\n",
        "        yield out_data_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 7.42 ms (started: 2021-10-25 13:20:06 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ8UpZjbeQ_8"
      },
      "source": [
        "#### 4.1.5 A perceptron classifier\n",
        "\n",
        "Construct a simple one hidden layer Perceptron."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJqS0vC7eQ_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d2d362c-6574-40d7-bceb-6fde8fded40d"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class ReviewClassifier(nn.Module):\n",
        "    \"\"\" a simple perceptron-based classifier \"\"\"\n",
        "    def __init__(self, num_features):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_features (int): the size of the input feature vector\n",
        "        \"\"\"\n",
        "        super(ReviewClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_features=num_features, out_features=1)\n",
        "    \n",
        "    def forward(self, x_in, apply_sigmoid=False):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor \n",
        "                    x_in.shape should be (batch, num_features)\n",
        "            apply_sigmoid (bool): a flag for the sigmoid activation\n",
        "                    should be false if used with the cross-entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch,).\n",
        "        \"\"\"\n",
        "        y_out = self.fc1(x_in).squeeze()\n",
        "        \n",
        "        if apply_sigmoid:\n",
        "            y_out = torch.sigmoid(y_out)\n",
        "        return y_out"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 14.3 ms (started: 2021-10-25 13:20:06 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvt-LHareRAA"
      },
      "source": [
        "### 4.2 Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scWKYJWfeRAB"
      },
      "source": [
        "#### 4.2.1 Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5NMMzfeeRAC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af020844-7401-4a4c-8eac-f049610fb59b"
      },
      "source": [
        "from argparse import Namespace\n",
        "args = Namespace(\n",
        "    # Data and path information\n",
        "    frequency_cutoff=25,\n",
        "    model_state_file='model.pth',\n",
        "    review_csv='../NLP/proj2_with_splits_task1.csv',\n",
        "    save_dir='../NLP/',\n",
        "    vectorizer_file='vectorizer.json',\n",
        "    # No model hyperparameters\n",
        "    # Training hyperparameters\n",
        "    batch_size=128,\n",
        "    early_stopping_criteria=5,\n",
        "    learning_rate=0.001,\n",
        "    num_epochs=100,\n",
        "    seed=1337,\n",
        "    # Runtime options\n",
        "    cuda=True,\n",
        "    device='cuda',\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 6.08 ms (started: 2021-10-25 13:20:06 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uazr39yZji5k",
        "outputId": "86a942b1-0812-497a-9675-13a6d78b79ff"
      },
      "source": [
        "# Get the Training, testing and validation dataset from the pre saved file\n",
        "review_csv='../NLP/proj2_with_splits_task1.csv' \n",
        "review_df = pd.read_csv(review_csv)\n",
        "type(review_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 195
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 23.7 ms (started: 2021-10-25 13:20:06 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6d6JELXeRAE"
      },
      "source": [
        "#### 4.2.2 Training Preparation\n",
        "\n",
        "Three key components for the training:\n",
        "\n",
        "- model\n",
        "- loss function\n",
        "- optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlxryRQCeRAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c79610d7-514e-484d-80fe-5df45380899f"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "\n",
        "def make_train_state(args):\n",
        "    return {'epoch_index': 0,\n",
        "            'train_loss': [],\n",
        "            'train_acc': [],\n",
        "            'val_loss': [],\n",
        "            'val_acc': [],\n",
        "            'test_loss': 1,\n",
        "            'test_acc': 1}\n",
        "\n",
        "train_state = make_train_state(args)\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "# dataset and vectorizer\n",
        "dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv)\n",
        "vectorizer = dataset.get_vectorizer()\n",
        "# model\n",
        "classifier = ReviewClassifier(num_features=len(vectorizer.review_vocab))\n",
        "classifier = classifier.to(args.device)\n",
        "# loss and optimizer\n",
        "loss_func = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 42.8 ms (started: 2021-10-25 13:20:06 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8jMz0ejeRAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90579cad-e913-442d-eb05-6b3eeb347017"
      },
      "source": [
        "def compute_accuracy(y_pred, y_target):\n",
        "    y_target = y_target.cpu()\n",
        "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()#.max(dim=1)[1]\n",
        "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
        "    return n_correct / len(y_pred_indices) * 100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.41 ms (started: 2021-10-25 13:20:06 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhUFfoLUeRAJ"
      },
      "source": [
        "#### 4.2.3 Training \n",
        "\n",
        "This is the most time-consuming section.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlDX-TVSeRAO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52688913-4d1c-41b1-f0c0-da0409569000"
      },
      "source": [
        "import numpy as np\n",
        "for epoch_index in range(args.num_epochs):\n",
        "    train_state['epoch_index'] = epoch_index\n",
        "    # Iterate over training dataset\n",
        "    # setup: batch generator, set loss and acc to 0, set train mode on\n",
        "    dataset.set_split('train')\n",
        "    batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    classifier.train()\n",
        "    for batch_index, batch_dict in enumerate(batch_generator):\n",
        "        # the training routine is 5 steps:\n",
        "        # step 1. zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # step 2. compute the output\n",
        "        y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
        "        # step 3. compute the loss\n",
        "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
        "        loss_batch = loss.item()\n",
        "        running_loss += (loss_batch-running_loss) / (batch_index + 1)\n",
        "        # step 4. use loss to produce gradients\n",
        "        loss.backward()\n",
        "        # step 5. use optimizer to take gradient step\n",
        "        optimizer.step()\n",
        "        # compute the accuracy\n",
        "        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
        "\n",
        "    train_state['train_loss'].append(running_loss)\n",
        "    train_state['train_acc'].append(running_acc)\n",
        "\n",
        "    # Iterate over val dataset\n",
        "    # setup: batch generator, set loss and acc to 0, set eval mode on\n",
        "    dataset.set_split('val')\n",
        "    batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
        "    running_loss = 0.\n",
        "    running_acc = 0.\n",
        "    classifier.eval()\n",
        "\n",
        "    for batch_index, batch_dict in enumerate(batch_generator):\n",
        "        # step 1. compute the output\n",
        "        y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
        "        # step 2. compute the loss\n",
        "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
        "        loss_batch = loss.item()\n",
        "        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
        "        # step 3. compute the accuracy\n",
        "        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "        running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
        "    train_state['val_loss'].append(running_loss)\n",
        "    train_state['val_acc'].append(running_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 21.5 s (started: 2021-10-25 13:20:06 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYSM-JLreRAQ"
      },
      "source": [
        "### 4.3 Evaluation, Inference, and Inspection\n",
        "After you have a trained model, the next steps are to either evaluate how it did against some heldout\n",
        "portion of the data, use it to do inference on new data, or inspect the model weights to see what it is has learned. In this section, we will show you all three steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCuHPCq1zVp7"
      },
      "source": [
        "#### 4.3.1 EVALUATING ON TEST DATA\n",
        "The simple feed-forward neural network model with one-hot-encoding as word embedding yielded an accuracy of 73.44, with 0.569 test loss and 21.5 seconds as training time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO2mhi1seRAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f24d0fb-4b52-4cac-a627-56231fb4b05f"
      },
      "source": [
        "dataset.set_split('test')\n",
        "batch_generator = generate_batches(dataset,batch_size=args.batch_size,device=args.device)\n",
        "running_loss = 0.\n",
        "running_acc = 0.\n",
        "classifier.eval()\n",
        "for batch_index, batch_dict in enumerate(batch_generator):\n",
        "    # compute the output\n",
        "    y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
        "    # compute the loss\n",
        "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
        "    loss_batch = loss.item()\n",
        "    running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
        "    # compute the accuracy\n",
        "    acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "    running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
        "train_state['test_loss'] = running_loss\n",
        "train_state['test_acc'] = running_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 47.9 ms (started: 2021-10-25 13:20:28 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04a0h07neRAS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50797dbe-574b-419b-e7c7-7538208492af"
      },
      "source": [
        "print(\"Test loss: {:.3f}\".format(train_state['test_loss']))\n",
        "print(\"Test Accuracy: {:.2f}\".format(train_state['test_acc']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.569\n",
            "Test Accuracy: 73.44\n",
            "time: 3.59 ms (started: 2021-10-25 13:20:28 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsAZSXxWCZTw"
      },
      "source": [
        "## 5 A CNN Conv1d based model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ61WeIidjlq"
      },
      "source": [
        "## GloVe\n",
        "1. For this task, we implemented GloVe as our word embedding method. It is a pre-trained embedding method which converts the text data into numerical vectors so that we could train our model on it.\n",
        "2. Different from one-hot-encoding, GloVe is a statistical based method . It performed on an aggregated global word-word co-occurrence matrix,  which produces a vector space with meaningful sub-structures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeJ3Bj4r0d26"
      },
      "source": [
        "### 5.1  CNN Conv1d based model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcA2m-G2CiT1"
      },
      "source": [
        "#### 5.1.1 CNN Vocabulary Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I-JTa1Cumnw"
      },
      "source": [
        "One-hot encoding for setting a vocabulary class to look up an index for a token and vice versa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCczZxsFCfwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f755fcb-d9fb-42a5-fb27-b274cc90779e"
      },
      "source": [
        "class Vocabulary(object):\n",
        "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
        "\n",
        "    def __init__(self, token_to_idx=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
        "        \"\"\"\n",
        "\n",
        "        if token_to_idx is None:\n",
        "            token_to_idx = {}\n",
        "        self._token_to_idx = token_to_idx\n",
        "\n",
        "        self._idx_to_token = {idx: token \n",
        "                              for token, idx in self._token_to_idx.items()}\n",
        "        \n",
        "    def to_serializable(self):\n",
        "        \"\"\" returns a dictionary that can be serialized \"\"\"\n",
        "        return {'token_to_idx': self._token_to_idx}\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n",
        "        return cls(**contents)\n",
        "\n",
        "    def add_token(self, token):\n",
        "        \"\"\"Update mapping dicts based on the token.\n",
        "\n",
        "        Args:\n",
        "            token (str): the item to add into the Vocabulary\n",
        "        Returns:\n",
        "            index (int): the integer corresponding to the token\n",
        "        \"\"\"\n",
        "        if token in self._token_to_idx:\n",
        "            index = self._token_to_idx[token]\n",
        "        else:\n",
        "            index = len(self._token_to_idx)\n",
        "            self._token_to_idx[token] = index\n",
        "            self._idx_to_token[index] = token\n",
        "        return index\n",
        "            \n",
        "    def add_many(self, tokens):\n",
        "        \"\"\"Add a list of tokens into the Vocabulary\n",
        "        \n",
        "        Args:\n",
        "            tokens (list): a list of string tokens\n",
        "        Returns:\n",
        "            indices (list): a list of indices corresponding to the tokens\n",
        "        \"\"\"\n",
        "        return [self.add_token(token) for token in tokens]\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        \"\"\"Retrieve the index associated with the token \n",
        "        \n",
        "        Args:\n",
        "            token (str): the token to look up \n",
        "        Returns:\n",
        "            index (int): the index corresponding to the token\n",
        "        \"\"\"\n",
        "        return self._token_to_idx[token]\n",
        "\n",
        "    def lookup_index(self, index):\n",
        "        \"\"\"Return the token associated with the index\n",
        "        \n",
        "        Args: \n",
        "            index (int): the index to look up\n",
        "        Returns:\n",
        "            token (str): the token corresponding to the index\n",
        "        Raises:\n",
        "            KeyError: if the index is not in the Vocabulary\n",
        "        \"\"\"\n",
        "        if index not in self._idx_to_token:\n",
        "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
        "        return self._idx_to_token[index]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._token_to_idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 40 ms (started: 2021-10-25 13:20:28 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB80iOTCClPU"
      },
      "source": [
        "#### 5.1.2 Sequence Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqWkTvJtCpei",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "365909ed-1b5f-474b-bddc-060eda39c500"
      },
      "source": [
        "class SequenceVocabulary(Vocabulary):\n",
        "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
        "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
        "                 end_seq_token=\"<END>\"):\n",
        "\n",
        "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
        "\n",
        "        self._mask_token = mask_token\n",
        "        self._unk_token = unk_token\n",
        "        self._begin_seq_token = begin_seq_token\n",
        "        self._end_seq_token = end_seq_token\n",
        "\n",
        "        self.mask_index = self.add_token(self._mask_token)\n",
        "        self.unk_index = self.add_token(self._unk_token)\n",
        "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
        "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
        "\n",
        "    def to_serializable(self):\n",
        "        contents = super(SequenceVocabulary, self).to_serializable()\n",
        "        contents.update({'unk_token': self._unk_token,\n",
        "                         'mask_token': self._mask_token,\n",
        "                         'begin_seq_token': self._begin_seq_token,\n",
        "                         'end_seq_token': self._end_seq_token})\n",
        "        return contents\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        \"\"\"Retrieve the index associated with the token \n",
        "          or the UNK index if token isn't present.\n",
        "        \n",
        "        Args:\n",
        "            token (str): the token to look up \n",
        "        Returns:\n",
        "            index (int): the index corresponding to the token\n",
        "        Notes:\n",
        "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
        "              for the UNK functionality \n",
        "        \"\"\"\n",
        "        if self.unk_index >= 0:\n",
        "            return self._token_to_idx.get(token, self.unk_index)\n",
        "        else:\n",
        "            return self._token_to_idx[token]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 19.4 ms (started: 2021-10-25 13:20:28 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIyec6OyFHdL"
      },
      "source": [
        "#### 5.1.3 The Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_kov3o2tta2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee23f9f0-d797-49a5-9c8a-40fb039403c7"
      },
      "source": [
        "class NarVectorizer(object): # Narrative\n",
        "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"    \n",
        "    def __init__(self, narrative_vocab, classes_vocab):\n",
        "        self.narrative_vocab = narrative_vocab\n",
        "        self.classes_vocab = classes_vocab\n",
        "\n",
        "    def vectorize(self, narrative, vector_length=-1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            narrative (str): the string of words separated by a space\n",
        "            vector_length (int): an argument for forcing the length of index vector\n",
        "        Returns:\n",
        "            the vetorized narrative (numpy.array)\n",
        "        \"\"\"\n",
        "        indices = [self.narrative_vocab.begin_seq_index]\n",
        "        indices.extend(self.narrative_vocab.lookup_token(token) \n",
        "                       for token in narrative.split(\" \"))\n",
        "        indices.append(self.narrative_vocab.end_seq_index)\n",
        "\n",
        "        if vector_length < 0:\n",
        "            vector_length = len(indices)\n",
        "\n",
        "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
        "        out_vector[:len(indices)] = indices\n",
        "        out_vector[len(indices):] = self.narrative_vocab.mask_index\n",
        "\n",
        "        return out_vector\n",
        "\n",
        "    @classmethod\n",
        "    def from_dataframe(cls, nar_df, cutoff=25):\n",
        "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
        "        \n",
        "        Args:\n",
        "            nar_df (pandas.DataFrame): the target dataset\n",
        "            cutoff (int): frequency threshold for including in Vocabulary \n",
        "        Returns:\n",
        "            an instance of the NarVectorizer\n",
        "        \"\"\"\n",
        "        classes_vocab = Vocabulary()        \n",
        "        for classes in sorted(set(nar_df.classes)):\n",
        "            classes_vocab.add_token(classes)\n",
        "\n",
        "        word_counts = Counter()\n",
        "        for narrative in nar_df.narrative:\n",
        "            for token in narrative.split(\" \"):\n",
        "                if token not in string.punctuation:\n",
        "                    word_counts[token] += 1\n",
        "        \n",
        "        narrative_vocab = SequenceVocabulary()\n",
        "        for word, word_count in word_counts.items():\n",
        "            if word_count >= cutoff:\n",
        "                narrative_vocab.add_token(word)\n",
        "        \n",
        "        return cls(narrative_vocab, classes_vocab)\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        narrative_vocab = \\\n",
        "            SequenceVocabulary.from_serializable(contents['narrative_vocab'])\n",
        "        classes_vocab =  \\\n",
        "            Vocabulary.from_serializable(contents['classes_vocab'])\n",
        "\n",
        "        return cls(narrative_vocab=narrative_vocab, classes_vocab=classes_vocab)\n",
        "\n",
        "    def to_serializable(self):\n",
        "        return {'narrative_vocab': self.narrative_vocab.to_serializable(),\n",
        "                'classes_vocab': self.classes_vocab.to_serializable()}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 40.2 ms (started: 2021-10-25 13:20:28 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3frCtEtvKzP"
      },
      "source": [
        "#### 5.1.4 The Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jhO3NtbvPke",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e623a33-21d4-4d1c-ae54-29899e0a7143"
      },
      "source": [
        "class NarDataset(Dataset):\n",
        "    def __init__(self, nar_df, vectorizer):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            nar_df (pandas.DataFrame): the dataset\n",
        "            vectorizer (NarVectorizer): vectorizer instatiated from dataset\n",
        "        \"\"\"\n",
        "        self.nar_df = nar_df\n",
        "        self._vectorizer = vectorizer\n",
        "\n",
        "        # +1 if only using begin_seq, +2 if using both begin and end seq tokens\n",
        "        measure_len = lambda context: len(context.split(\" \"))\n",
        "        self._max_seq_length = max(map(measure_len, nar_df.narrative)) + 2\n",
        "        \n",
        "\n",
        "        self.train_df = self.nar_df[self.nar_df.split=='train']\n",
        "        self.train_size = len(self.train_df)\n",
        "\n",
        "        self.val_df = self.nar_df[self.nar_df.split=='val']\n",
        "        self.validation_size = len(self.val_df)\n",
        "\n",
        "        self.test_df = self.nar_df[self.nar_df.split=='test']\n",
        "        self.test_size = len(self.test_df)\n",
        "\n",
        "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
        "                             'val': (self.val_df, self.validation_size),\n",
        "                             'test': (self.test_df, self.test_size)}\n",
        "\n",
        "        self.set_split('train')\n",
        "\n",
        "        # Class weights\n",
        "        class_counts = nar_df.classes.value_counts().to_dict()\n",
        "        def sort_key(item):\n",
        "            return self._vectorizer.classes_vocab.lookup_token(item[0])\n",
        "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
        "        frequencies = [count for _, count in sorted_counts]\n",
        "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
        "        \n",
        "        \n",
        "    @classmethod\n",
        "    def load_dataset_and_make_vectorizer(cls, nar_csv):\n",
        "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
        "        \n",
        "        Args:\n",
        "            nar_csv (str): location of the dataset\n",
        "        Returns:\n",
        "            an instance of NarDataset\n",
        "        \"\"\"\n",
        "        nar_df = pd.read_csv(nar_csv)\n",
        "        train_nar_df = nar_df[nar_df.split=='train']\n",
        "        return cls(nar_df, NarVectorizer.from_dataframe(train_nar_df))\n",
        "\n",
        "    @classmethod\n",
        "    def load_dataset_and_load_vectorizer(cls, nar_csv, vectorizer_filepath):\n",
        "        \"\"\"Load dataset and the corresponding vectorizer. \n",
        "        Used in the case in the vectorizer has been cached for re-use\n",
        "        \n",
        "        Args:\n",
        "            nar_csv (str): location of the dataset\n",
        "            vectorizer_filepath (str): location of the saved vectorizer\n",
        "        Returns:\n",
        "            an instance of NarDataset\n",
        "        \"\"\"\n",
        "        nar_df = pd.read_csv(nar_csv)\n",
        "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
        "        return cls(nar_csv, vectorizer)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_vectorizer_only(vectorizer_filepath):\n",
        "        \"\"\"a static method for loading the vectorizer from file\n",
        "        \n",
        "        Args:\n",
        "            vectorizer_filepath (str): the location of the serialized vectorizer\n",
        "        Returns:\n",
        "            an instance of NarVectorizer\n",
        "        \"\"\"\n",
        "        with open(vectorizer_filepath) as fp:\n",
        "            return NameVectorizer.from_serializable(json.load(fp))\n",
        "\n",
        "    def save_vectorizer(self, vectorizer_filepath):\n",
        "        \"\"\"saves the vectorizer to disk using json\n",
        "        \n",
        "        Args:\n",
        "            vectorizer_filepath (str): the location to save the vectorizer\n",
        "        \"\"\"\n",
        "        with open(vectorizer_filepath, \"w\") as fp:\n",
        "            json.dump(self._vectorizer.to_serializable(), fp)\n",
        "\n",
        "    def get_vectorizer(self):\n",
        "        \"\"\" returns the vectorizer \"\"\"\n",
        "        return self._vectorizer\n",
        "\n",
        "    def set_split(self, split=\"train\"):\n",
        "        \"\"\" selects the splits in the dataset using a column in the dataframe \"\"\"\n",
        "        self._target_split = split\n",
        "        self._target_df, self._target_size = self._lookup_dict[split]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._target_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"the primary entry point method for PyTorch datasets\n",
        "        \n",
        "        Args:\n",
        "            index (int): the index to the data point \n",
        "        Returns:\n",
        "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
        "        \"\"\"\n",
        "        row = self._target_df.iloc[index]\n",
        "\n",
        "        narrative_vector = \\\n",
        "            self._vectorizer.vectorize(row.narrative, self._max_seq_length)\n",
        "\n",
        "        classes_index = \\\n",
        "            self._vectorizer.classes_vocab.lookup_token(row.classes)\n",
        "\n",
        "        return {'x_data': narrative_vector,\n",
        "                'y_target': classes_index}\n",
        "\n",
        "    def get_num_batches(self, batch_size):\n",
        "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
        "        \n",
        "        Args:\n",
        "            batch_size (int)\n",
        "        Returns:\n",
        "            number of batches in the dataset\n",
        "        \"\"\"\n",
        "        return len(self) // batch_size\n",
        "\n",
        "def generate_batches(dataset, batch_size, shuffle=True,\n",
        "                     drop_last=True, device=\"cpu\"): \n",
        "    \"\"\"\n",
        "    A generator function which wraps the PyTorch DataLoader. It will \n",
        "      ensure each tensor is on the write device location.\n",
        "    \"\"\"\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
        "                            shuffle=shuffle, drop_last=drop_last)\n",
        "\n",
        "    for data_dict in dataloader:\n",
        "        out_data_dict = {}\n",
        "        for name, tensor in data_dict.items():\n",
        "            out_data_dict[name] = data_dict[name].to(device)\n",
        "        yield out_data_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 122 ms (started: 2021-10-25 13:20:28 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gXRtQdAxHkF"
      },
      "source": [
        "#### 5.1.5 The Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyVj-lKEyC-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1a557b4-6f52-49ad-f007-3d3cb02ce300"
      },
      "source": [
        "class NarClassifier(nn.Module):\n",
        "    def __init__(self, embedding_size, num_embeddings, num_channels, \n",
        "                 hidden_dim, num_classes, dropout_p, \n",
        "                 pretrained_embeddings=None, padding_idx=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding_size (int): size of the embedding vectors\n",
        "            num_embeddings (int): number of embedding vectors\n",
        "            filter_width (int): width of the convolutional kernels\n",
        "            num_channels (int): number of convolutional kernels per layer\n",
        "            hidden_dim (int): the size of the hidden dimension\n",
        "            num_classes (int): the number of classes in classification\n",
        "            dropout_p (float): a dropout parameter \n",
        "            pretrained_embeddings (numpy.array): previously trained word embeddings\n",
        "                default is None. If provided, \n",
        "            padding_idx (int): an index representing a null position\n",
        "        \"\"\"\n",
        "        super(NarClassifier, self).__init__()\n",
        "\n",
        "        if pretrained_embeddings is None:\n",
        "\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx)        \n",
        "        else:\n",
        "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
        "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
        "                                    num_embeddings=num_embeddings,\n",
        "                                    padding_idx=padding_idx,\n",
        "                                    _weight=pretrained_embeddings)\n",
        "        \n",
        "            \n",
        "        self.convnet = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=embedding_size, \n",
        "                   out_channels=num_channels, kernel_size=3),\n",
        "            nn.ELU(),\n",
        "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
        "                   kernel_size=3, stride=2),\n",
        "            nn.ELU(),\n",
        "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
        "                   kernel_size=3, stride=2),\n",
        "            nn.ELU(),\n",
        "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
        "                   kernel_size=3),\n",
        "            nn.ELU()\n",
        "        )\n",
        "\n",
        "        self._dropout_p = dropout_p\n",
        "        self.fc1 = nn.Linear(num_channels, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x_in, apply_softmax=False):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "        \n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor. \n",
        "                x_in.shape should be (batch, dataset._max_seq_length)\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, num_classes)\n",
        "        \"\"\"\n",
        "        \n",
        "        # embed and permute so features are channels\n",
        "        x_embedded = self.emb(x_in).permute(0, 2, 1)\n",
        "\n",
        "        features = self.convnet(x_embedded)\n",
        "\n",
        "        # average and remove the extra dimension\n",
        "        remaining_size = features.size(dim=2)\n",
        "        features = F.avg_pool1d(features, remaining_size).squeeze(dim=2)\n",
        "        features = F.dropout(features, p=self._dropout_p)\n",
        "        \n",
        "        # mlp classifier\n",
        "        intermediate_vector = F.relu(F.dropout(self.fc1(features), p=self._dropout_p))\n",
        "        prediction_vector = self.fc2(intermediate_vector)\n",
        "\n",
        "        if apply_softmax:\n",
        "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
        "\n",
        "        return prediction_vector"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 44.1 ms (started: 2021-10-25 13:20:28 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHW-CcSXyW9a"
      },
      "source": [
        "### 5.2 CNN Training Routine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0gyX2LkyObX"
      },
      "source": [
        "#### 5.2.1 Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFFbrLurzAPf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "762ed582-8f53-4fc4-eecb-0c630e3ded3a"
      },
      "source": [
        "def make_train_state(args):\n",
        "    return {'stop_early': False,\n",
        "            'early_stopping_step': 0,\n",
        "            'early_stopping_best_val': 1e8,\n",
        "            'learning_rate': args.learning_rate,\n",
        "            'epoch_index': 0,\n",
        "            'train_loss': [],\n",
        "            'train_acc': [],\n",
        "            'val_loss': [],\n",
        "            'val_acc': [],\n",
        "            'test_loss': -1,\n",
        "            'test_acc': -1,\n",
        "            'model_filename': args.model_state_file}\n",
        "\n",
        "def update_train_state(args, model, train_state):\n",
        "    \"\"\"Handle the training state updates.\n",
        "\n",
        "    Components:\n",
        "     - Early Stopping: Prevent overfitting.\n",
        "     - Model Checkpoint: Model is saved if the model is better\n",
        "\n",
        "    :param args: main arguments\n",
        "    :param model: model to train\n",
        "    :param train_state: a dictionary representing the training state values\n",
        "    :returns:\n",
        "        a new train_state\n",
        "    \"\"\"\n",
        "\n",
        "    # Save one model at least\n",
        "    if train_state['epoch_index'] == 0:\n",
        "        torch.save(model.state_dict(), train_state['model_filename'])\n",
        "        train_state['stop_early'] = False\n",
        "\n",
        "    # Save model if performance improved\n",
        "    elif train_state['epoch_index'] >= 1:\n",
        "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
        "\n",
        "        # If loss worsened\n",
        "        if loss_t >= train_state['early_stopping_best_val']:\n",
        "            # Update step\n",
        "            train_state['early_stopping_step'] += 1\n",
        "        # Loss decreased\n",
        "        else:\n",
        "            # Save the best model\n",
        "            if loss_t < train_state['early_stopping_best_val']:\n",
        "                torch.save(model.state_dict(), train_state['model_filename'])\n",
        "\n",
        "            # Reset early stopping step\n",
        "            train_state['early_stopping_step'] = 0\n",
        "\n",
        "        # Stop early ?\n",
        "        train_state['stop_early'] = \\\n",
        "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
        "\n",
        "    return train_state\n",
        "\n",
        "def compute_accuracy(y_pred, y_target):\n",
        "    _, y_pred_indices = y_pred.max(dim=1)\n",
        "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
        "    return n_correct / len(y_pred_indices) * 100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 28.8 ms (started: 2021-10-25 13:20:28 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u30Z1M6z9kRL"
      },
      "source": [
        "#### 5.2.2 General Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92I5zQDN9jzB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dbddfcb-d644-4923-d18d-c3a4d312b821"
      },
      "source": [
        "def set_seed_everywhere(seed, cuda):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if cuda:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def handle_dirs(dirpath):\n",
        "    if not os.path.exists(dirpath):\n",
        "        os.makedirs(dirpath)\n",
        "        \n",
        "def load_glove_from_file(glove_filepath):\n",
        "    \"\"\"\n",
        "    Load the GloVe embeddings \n",
        "    \n",
        "    Args:\n",
        "        glove_filepath (str): path to the glove embeddings file \n",
        "    Returns:\n",
        "        word_to_index (dict), embeddings (numpy.ndarary)\n",
        "    \"\"\"\n",
        "\n",
        "    word_to_index = {}\n",
        "    embeddings = []\n",
        "    with open(glove_filepath, encoding=\"utf8\") as fp:\n",
        "        for index, line in enumerate(fp):\n",
        "            line = line.split(\" \") # each line: word num1 num2 ...\n",
        "            word_to_index[line[0]] = index # word = line[0] \n",
        "            embedding_i = np.array([float(val) for val in line[1:]])\n",
        "            embeddings.append(embedding_i)\n",
        "    return word_to_index, np.stack(embeddings)\n",
        "\n",
        "def make_embedding_matrix(glove_filepath, words):\n",
        "    \"\"\"\n",
        "    Create embedding matrix for a specific set of words.\n",
        "    \n",
        "    Args:\n",
        "        glove_filepath (str): file path to the glove embeddigns\n",
        "        words (list): list of words in the dataset\n",
        "    \"\"\"\n",
        "    word_to_idx, glove_embeddings = load_glove_from_file(glove_filepath)\n",
        "    embedding_size = glove_embeddings.shape[1]\n",
        "    \n",
        "    final_embeddings = np.zeros((len(words), embedding_size))\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "        if word in word_to_idx:\n",
        "            final_embeddings[i, :] = glove_embeddings[word_to_idx[word]]\n",
        "        else:\n",
        "            embedding_i = torch.ones(1, embedding_size)\n",
        "            torch.nn.init.xavier_uniform_(embedding_i)\n",
        "            final_embeddings[i, :] = embedding_i\n",
        "\n",
        "    return final_embeddings"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 26.7 ms (started: 2021-10-25 13:20:28 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VguiXyRKy_ml"
      },
      "source": [
        "#### 5.2.3 Settings and some prep work\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keFc7vM59yVl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa18b4db-9426-4f41-c502-36e41fa69945"
      },
      "source": [
        "args = Namespace(\n",
        "    # Data and Path hyper parameters\n",
        "    nar_csv=\"../NLP/proj2_with_splits_task1.csv\", # Modify to your filepath\n",
        "    vectorizer_file=\"vectorizer.json\",\n",
        "    model_state_file=\"model.pth\",\n",
        "    save_dir=\"model_storage\\document_classification\",\n",
        "    # Model hyper parameters\n",
        "    glove_filepath='../NLP/glove.6B.100d.txt',  # The glove 42B tokens and 300d vectors, Download from https://nlp.stanford.edu/projects/glove/\n",
        "    use_glove=True,\n",
        "    embedding_size=100, \n",
        "    hidden_dim=100, \n",
        "    num_channels=100, \n",
        "    # Training hyper parameter\n",
        "    seed=1337, \n",
        "    learning_rate=0.001, \n",
        "    dropout_p=0.0001, \n",
        "    batch_size=128, \n",
        "    num_epochs=300, \n",
        "    early_stopping_criteria=5, \n",
        "    # Runtime option\n",
        "    cuda=True, \n",
        "    catch_keyboard_interrupt=True, \n",
        "    reload_from_files=False,\n",
        "    expand_filepaths_to_save_dir=True,\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        ") \n",
        "\n",
        "if args.expand_filepaths_to_save_dir:\n",
        "    args.vectorizer_file = os.path.join(args.save_dir,\n",
        "                                        args.vectorizer_file)\n",
        "\n",
        "    args.model_state_file = os.path.join(args.save_dir,\n",
        "                                         args.model_state_file)\n",
        "    \n",
        "    print(\"Expanded filepaths: \")\n",
        "    print(\"\\t{}\".format(args.vectorizer_file))\n",
        "    print(\"\\t{}\".format(args.model_state_file))\n",
        "    \n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "    \n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "print(\"Using CUDA: {}\".format(args.cuda))\n",
        "\n",
        "# Set seed for reproducibility\n",
        "set_seed_everywhere(args.seed, args.cuda)\n",
        "\n",
        "# handle dirs\n",
        "handle_dirs(args.save_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expanded filepaths: \n",
            "\tmodel_storage\\document_classification/vectorizer.json\n",
            "\tmodel_storage\\document_classification/model.pth\n",
            "Using CUDA: False\n",
            "time: 36.6 ms (started: 2021-10-25 13:20:28 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA6yywH-AjLq"
      },
      "source": [
        "#### 5.2.4 Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8Zj8wm22yaA"
      },
      "source": [
        "This code is introducing glove file (if True) into our model, and it will take a few minutes (5-7 mins for glove.42B.300d, 16s for glove.6B.100d, depends on the performance and the size of glove file), and probably will take up most of the RAM of Colab or your own computer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAegb2TAAptE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9690807-2a4b-48ad-9dd6-436ccdda84ea"
      },
      "source": [
        "# Use Glove for embeddings\n",
        "args.use_glove = True\n",
        "\n",
        "if args.reload_from_files:\n",
        "    # training from a checkpoint\n",
        "    dataset = NarDataset.load_dataset_and_load_vectorizer(args.nar_csv, args.vectorizer_file)\n",
        "else:\n",
        "    # create dataset and vectorizer\n",
        "    dataset = NarDataset.load_dataset_and_make_vectorizer(args.nar_csv)\n",
        "    dataset.save_vectorizer(args.vectorizer_file)\n",
        "vectorizer = dataset.get_vectorizer()\n",
        "\n",
        "# Use GloVe or randomly initialized embeddings\n",
        "if args.use_glove:\n",
        "    words = vectorizer.narrative_vocab._token_to_idx.keys()\n",
        "    embeddings = make_embedding_matrix(glove_filepath=args.glove_filepath, \n",
        "                                       words=words)\n",
        "    print(\"Using pre-trained embeddings\")\n",
        "else:\n",
        "    print(\"Not using pre-trained embeddings\")\n",
        "    embeddings = None\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using pre-trained embeddings\n",
            "time: 17.7 s (started: 2021-10-25 13:20:28 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPuk3hb7FZXA"
      },
      "source": [
        "Call the args into classifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVjAA9FRFXJm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1be1647e-1c33-465e-91b5-5f0ae6563a56"
      },
      "source": [
        "classifier = NarClassifier(embedding_size=args.embedding_size, \n",
        "                            num_embeddings=len(vectorizer.narrative_vocab),\n",
        "                            num_channels=args.num_channels,\n",
        "                            hidden_dim=args.hidden_dim, \n",
        "                            num_classes=len(vectorizer.classes_vocab), \n",
        "                            dropout_p=args.dropout_p,\n",
        "                            pretrained_embeddings=embeddings,\n",
        "                            padding_idx=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 9.44 ms (started: 2021-10-25 13:20:46 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHXij6Tuo_mV"
      },
      "source": [
        "#### 5.2.5 Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48etbpjUnHBr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130,
          "referenced_widgets": [
            "b4296aec38164b7496621245c08b82c9",
            "5f5b50d1d8ae4edb9c0da05172ded165",
            "19df89a42a064c3e98712a92b86e9705",
            "04ae32793d6e4014bf9c91ebf8a8bc8c",
            "d1de220393e243de952826290d00f97a",
            "c66ddda8ec0c49b1805258a990d1f69c",
            "443aaccb37764d51ac444b8d049f58c6",
            "70e8fb64d1d74dcfb0b418bc3c890c0e",
            "8525eba212424845bd0dac24f8c75565",
            "394fd9816bf640eebd1ecc46e633a65f",
            "9fc81d6896b74082916f79882bdaf6f4",
            "c44b3f54d84e453991f9f5a32608304e",
            "aeecac282ed449a3bfa83f56afb0f675",
            "2c9582248ac14368befb4b15f620c383",
            "f46d4b7bbcc44d128bfef287cad6fc03",
            "a907b8737d4f4c328a14106f5bc5fe55",
            "d46dfc68643a49f983222849b965506e",
            "7d92b6f6087441b7ac872b0f663c7c53",
            "b72b006d65e446539497246c1cd9d8d7",
            "09a33735d32c428e812c7c410f341251",
            "97ab1e75a9544590b030b0c204638d64",
            "56638dfec384461f8634c8a4f5a6b275",
            "28be81ad5bce49a4a42f5fdab9c29b0f",
            "c2b9981ae1d64f26a7ba9ab8d1d06097",
            "3206065547054ed4b6fe9862e6aa3da5",
            "207b758667084744823048741960e7e7",
            "b6b3815303ca4b95bbbe2888c6b0de73",
            "b2cbfc5e1da24a5587164d0c94b13635",
            "361a9b89bfd74367a1e4612dc8c6b3bd",
            "9ff55aadc1f841f8bb16a579c758b024",
            "85c30d015cf146628ab50a6f1f62b5bc",
            "555790f7b8544204aabee93e6e17cf1c",
            "8443a4329b554a04afe71b5d44dfccbf"
          ]
        },
        "outputId": "f86aa4ac-a231-4ea8-c16a-3257d55e712e"
      },
      "source": [
        "classifier = classifier.to(args.device)\n",
        "dataset.class_weights = dataset.class_weights.to(args.device)\n",
        "    \n",
        "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
        "                                           mode='min', factor=0.5,\n",
        "                                           patience=1)\n",
        "\n",
        "train_state = make_train_state(args)\n",
        "\n",
        "epoch_bar = tqdm(desc='training routine', \n",
        "                          total=args.num_epochs,\n",
        "                          position=0)\n",
        "\n",
        "dataset.set_split('train')\n",
        "train_bar = tqdm(desc='split=train',\n",
        "                          total=dataset.get_num_batches(args.batch_size), \n",
        "                          position=1, \n",
        "                          leave=True)\n",
        "dataset.set_split('val')\n",
        "val_bar = tqdm(desc='split=val',\n",
        "                        total=dataset.get_num_batches(args.batch_size), \n",
        "                        position=1, \n",
        "                        leave=True)\n",
        "\n",
        "try:\n",
        "    for epoch_index in range(args.num_epochs):\n",
        "        train_state['epoch_index'] = epoch_index\n",
        "\n",
        "        # Iterate over training dataset\n",
        "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
        "\n",
        "        dataset.set_split('train')\n",
        "        batch_generator = generate_batches(dataset, \n",
        "                                           batch_size=args.batch_size, \n",
        "                                           device=args.device)\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "        classifier.train()\n",
        "\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            # the training routine is these 5 steps:\n",
        "            # --------------------------------------\n",
        "            # step 1. zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # step 2. compute the output\n",
        "            y_pred = classifier(batch_dict['x_data'])\n",
        "\n",
        "            # step 3. compute the loss\n",
        "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
        "            loss_t = loss.item()\n",
        "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "            # step 4. use loss to produce gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # step 5. use optimizer to take gradient step\n",
        "            optimizer.step()\n",
        "            # -----------------------------------------\n",
        "            # compute the accuracy\n",
        "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "            # update bar\n",
        "            train_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
        "                                  epoch=epoch_index)\n",
        "            train_bar.update()\n",
        "\n",
        "        train_state['train_loss'].append(running_loss)\n",
        "        train_state['train_acc'].append(running_acc)\n",
        "\n",
        "        # Iterate over val dataset\n",
        "\n",
        "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
        "        dataset.set_split('val')\n",
        "        batch_generator = generate_batches(dataset, \n",
        "                                           batch_size=args.batch_size, \n",
        "                                           device=args.device)\n",
        "        running_loss = 0.\n",
        "        running_acc = 0.\n",
        "        classifier.eval()\n",
        "\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "\n",
        "            # compute the output\n",
        "            y_pred =  classifier(batch_dict['x_data'])\n",
        "\n",
        "            # step 3. compute the loss\n",
        "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
        "            loss_t = loss.item()\n",
        "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "            # compute the accuracy\n",
        "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "            val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
        "                            epoch=epoch_index)\n",
        "            val_bar.update()\n",
        "\n",
        "        train_state['val_loss'].append(running_loss)\n",
        "        train_state['val_acc'].append(running_acc)\n",
        "\n",
        "        train_state = update_train_state(args=args, model=classifier,\n",
        "                                         train_state=train_state)\n",
        "\n",
        "        scheduler.step(train_state['val_loss'][-1])\n",
        "\n",
        "        if train_state['stop_early']:\n",
        "            break\n",
        "\n",
        "        train_bar.n = 0\n",
        "        val_bar.n = 0\n",
        "        epoch_bar.update()\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Exiting loop\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4296aec38164b7496621245c08b82c9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "training routine:   0%|          | 0/300 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c44b3f54d84e453991f9f5a32608304e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "split=train:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28be81ad5bce49a4a42f5fdab9c29b0f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "split=val:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 6min 3s (started: 2021-10-25 13:20:46 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFwjz5KM9y7x"
      },
      "source": [
        "#### Compute the loss and accuracy\n",
        "After several times of tunning hyper parameters (learning rate and drop_rate), we find the relatively good accuracy when\n",
        "lr = 0.001, dr = 0.0001, where Test loss: 0.65 and Test Accuracy: 71.875%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EWStGbGvVFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c03b9f7-fd7c-451c-ade4-7df3314f5b9b"
      },
      "source": [
        "# compute the loss & accuracy on the test set using the best available model\n",
        "\n",
        "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
        "\n",
        "classifier = classifier.to(args.device)\n",
        "dataset.class_weights = dataset.class_weights.to(args.device)\n",
        "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
        "\n",
        "dataset.set_split('test')\n",
        "batch_generator = generate_batches(dataset, \n",
        "                                   batch_size=args.batch_size, \n",
        "                                   device=args.device)\n",
        "running_loss = 0.\n",
        "running_acc = 0.\n",
        "classifier.eval()\n",
        "\n",
        "for batch_index, batch_dict in enumerate(batch_generator):\n",
        "    # compute the output\n",
        "    y_pred =  classifier(batch_dict['x_data'])\n",
        "    \n",
        "    # compute the loss\n",
        "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
        "    loss_t = loss.item()\n",
        "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "    # compute the accuracy\n",
        "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "train_state['test_loss'] = running_loss\n",
        "train_state['test_acc'] = running_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 211 ms (started: 2021-10-25 13:26:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Smdg98bGUdK7"
      },
      "source": [
        "#### Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9ciC5ypvYZ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "687c4f43-e9af-4255-cd89-fe0f483b7cfa"
      },
      "source": [
        "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
        "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.6507607698440552;\n",
            "Test Accuracy: 71.875\n",
            "time: 3.37 ms (started: 2021-10-25 13:26:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHlQQeXkvbds"
      },
      "source": [
        "#### 5.2.6 Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPFcR1Kov2m_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93dfd931-578f-4304-ad0d-e51248e3cafb"
      },
      "source": [
        "# Preprocess the text\n",
        "def preprocess_text(text):\n",
        "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
        "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
        "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.92 ms (started: 2021-10-25 13:26:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6OJkJBvvapE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f79c441-9be7-440d-96d0-e5563cb2e721"
      },
      "source": [
        "def predict_classes(title, classifier, vectorizer, max_length):\n",
        "    \"\"\"Predict a News class for the degree of injury\n",
        "    \n",
        "    Args:\n",
        "        title (str): a raw NARRATIVE string\n",
        "        classifier (NewsClassifier): an instance of the trained classifier\n",
        "        vectorizer (NewsVectorizer): the corresponding vectorizer\n",
        "        max_length (int): the max sequence length\n",
        "            Note: CNNs are sensitive to the input data tensor size. \n",
        "                  This ensures to keep it the same size as the training data\n",
        "    \"\"\"\n",
        "    title = preprocess_text(title)\n",
        "    vectorized_title = \\\n",
        "        torch.tensor(vectorizer.vectorize(title, vector_length=max_length))\n",
        "    result = classifier(vectorized_title.unsqueeze(0), apply_softmax=True)\n",
        "    probability_values, indices = result.max(dim=1)\n",
        "    predicted_classes = vectorizer.classes_vocab.lookup_index(indices.item())\n",
        "\n",
        "    return {'classes': predicted_classes, \n",
        "            'probability': probability_values.item()}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 7.91 ms (started: 2021-10-25 13:26:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8wRqFEkv6EG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01ec600e-8217-4bcf-ea44-03ae1267976d"
      },
      "source": [
        "def get_samples():\n",
        "    samples = {}\n",
        "    for cat in dataset.val_df.classes.unique():\n",
        "        samples[cat] = dataset.val_df.narrative[dataset.val_df.classes==cat].tolist()[:5]\n",
        "    return samples\n",
        "\n",
        "val_samples = get_samples()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 6.65 ms (started: 2021-10-25 13:26:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REiOcerYv8DK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7874ccdb-d797-44cb-eb8f-eafccf50b96a"
      },
      "source": [
        "#title = input(\"Enter a news title to classify: \")\n",
        "classifier = classifier.to(\"cpu\")\n",
        "\n",
        "for truth, sample_group in val_samples.items():\n",
        "    print(f\"True Classes: {truth}\")\n",
        "    print(\"=\"*30)\n",
        "    for sample in sample_group:\n",
        "        prediction = predict_classes(sample, classifier, \n",
        "                                      vectorizer, dataset._max_seq_length + 1)\n",
        "        print(\"Prediction: {} (p={:0.2f})\".format(prediction['classes'],\n",
        "                                                  prediction['probability']))\n",
        "        print(\"\\t + Sample: {}\".format(sample))\n",
        "    print(\"-\"*30 + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Classes: light\n",
            "==============================\n",
            "Prediction: severe (p=0.86)\n",
            "\t + Sample: ee right index finger injure hit hammer hold wrench ee clinic andit determine finger fracture \n",
            "Prediction: light (p=0.64)\n",
            "\t + Sample: salaried coordinatorplanner sit desk office service building felt right eye small spec material remove prescription med ication use \n",
            "Prediction: light (p=0.60)\n",
            "\t + Sample: shovel rock mud east line fee conveyor scale hard hat fell shovel bent pick hard hat straighten struck head beam small laceration head \n",
            "Prediction: light (p=0.91)\n",
            "\t + Sample: employee grind bracket hold line boring bearing production truck spark piece slag land chest area bounce grind shield safety glass land right eye receive prescription \n",
            "Prediction: light (p=0.62)\n",
            "\t + Sample: employee use hand grinder hold brass fitting left hand grinder slip fit strike left ring finger stitch update tetnus \n",
            "------------------------------\n",
            "\n",
            "True Classes: severe\n",
            "==============================\n",
            "Prediction: severe (p=0.80)\n",
            "\t + Sample: mechanic strap shovel drive shaft steam clean drive shaft pallet mechanic trip strap pallet fell concrete floor left shoulder \n",
            "Prediction: light (p=0.84)\n",
            "\t + Sample: work apron feeder instal skirt liner raise liner use pony pry bar liner raise bar slip liner fell catch left ring finger liner flight track pad break finger \n",
            "Prediction: severe (p=0.98)\n",
            "\t + Sample: employee truck shop stall step diagonal step loader right knee lock bent position doctor place restriction \n",
            "Prediction: severe (p=0.73)\n",
            "\t + Sample: weather blizzard condition wind snow slippery employee slip loader ladder land left heel bruise leave heel midified duty \n",
            "Prediction: severe (p=0.52)\n",
            "\t + Sample: loader operator away bin roll victim leg \n",
            "------------------------------\n",
            "\n",
            "time: 35.2 ms (started: 2021-10-25 13:26:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnHRx8ZWwOUU"
      },
      "source": [
        "## 6 Model Comparison\n",
        "The simple feed-forward neural network model with one-hot-encoding as word embedding yielded an accuracy of 73.44, with 0.569 test loss and 21.5 seconds as training time.\n",
        "The CNN Conv1d based model with GloVe as word embedding yielded an accuracy of 71.875, with 0.65 test loss and 6 min 3 seconds as training time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhFTklMQwOHC"
      },
      "source": [
        "### 6.1 Training Time\n",
        "* The simple feed-forward neural network model:21.5 seconds\n",
        "* The CNN Conv1d based model: 6 min 3 seconds\n",
        "<br>\n",
        "From the perspective of training time, the simple feed-forward neural network is a lot faster than the CNN Conv1d based model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47rniQKPwN6_"
      },
      "source": [
        "### 6.2 Test Loss\n",
        "* The simple feed-forward neural network model:0.569\n",
        "* The CNN Conv1d based model: 0.65 \n",
        "<br>\n",
        "<br>\n",
        "As loss is a summation of the errors which calculated based on the training and validation set with the neural networks. The objective of the learning model is to minimize the value of loss function. Therefore, a lower loss is desirable as it indicates a better model.\n",
        "<br>\n",
        "In this case, the test loss is lower for the simple feed-forward neural network model than the CNN Conv1d based model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SkRiG1XwNsu"
      },
      "source": [
        "### 6.3 Accuracy\n",
        "* The simple feed-forward neural network model:73.44%\n",
        "* The CNN Conv1d based model: 71.88%\n",
        "<br>\n",
        "<br>\n",
        "After the weights and parameters are learned (trained), we use the trained model to conduct the classification and use accuracy to represent the percentage of misclassification.\n",
        "<br>\n",
        "In this case, the accuracy of the simple feed-forward neural network model is slightly higher than the CNN Conv1d based model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrgqgPuYwNed"
      },
      "source": [
        "### 6.4 Conclusion\n",
        "From the analysis above, we can see that no matter it's training time, test loss or accuracy, the simple feed-forward neural network model performs better than the CNN Conv1d based model. Although the outcome is considered not intuitive:\n",
        "1. GloVe is considered a better word embedding method than one-hot-encoding.Other than the reasons stated earlier,  one-hot-encoding has two obvious disadvanges: curse of dimensionality and capture no information about the context.\n",
        "2. CNN Conv1d based model is more complex than simple NN, it has convolutional layer which can capture spatial relation. It is considered more effective in capturing information.\n",
        "<br>\n",
        "<br>\n",
        "In this case, our theory of why simple NN with one-hot-encoder is better than the CNN Conv1d based model with GloVe is that we used a relatively small pre-trained GloVe package which was pre-trained on more general text. For this specific dataset (injury in mining company), GloVe doesn't perform as good as one-hot-encoding which is conducted specificly based on this smaller dataset (2000 records).\n",
        "If we use a bigger GloVe package with more records, the performance might improve. \n"
      ]
    }
  ]
}